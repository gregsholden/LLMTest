{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 20479\n",
      "I HAD alw\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and review the first 100 characters\n",
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Length of text:\", len(raw_text))\n",
    "print(raw_text[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3]\n",
    "tens1 = torch.tensor(data)\n",
    "print(tens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "array = np.array([1,2,3])\n",
    "print(array)\n",
    "tens2 = torch.from_numpy(array)\n",
    "print(tens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ch 2, p 22 - split text\n",
    "import re\n",
    "text = \"Hello, world.  This, is a test.\"\n",
    "result = re.split(r'([!,.]|\\s)', text)\n",
    "print(result)\n",
    "result2 = [item for item in result if item.strip()]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Add more puncuation and  --- to the split (p 23)\n",
    "text = \"Hello, world.  Is this-- a test?\"\n",
    "result = re.split(r'([!,.?:]|--|\\s)', text)\n",
    "print(result)\n",
    "result2 = [item for item in result if item.strip()]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "# p 24 - Use tokenizer scheme on Wharton text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Test output of tokenized used on The Verdict  \n",
    "print(preprocessed[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "['yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# 2.3 - Create a vocabulary (p 25)\n",
    "\n",
    "# Create list of all unique words and sort\n",
    "all_words   = sorted(set(preprocessed))\n",
    "vocab_size  = len(all_words)\n",
    "print(vocab_size)\n",
    "print(all_words[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Use vocab to create a dictionary\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 27 - Make simple tokenizer Class\n",
    "class SimpleTokenV1:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores the vocab dictionary\n",
    "        self.int_to_str = {integer: string for string, integer in vocab.items()} # reverse dictionary (token IDs to text tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids # return the token IDs\n",
    "\n",
    "    def decoder(self, ids):  # Converts token IDs back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "tokenizer = SimpleTokenV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# Need to account for words not in the vocab (p 30)\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that new characters are in the vocab\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 31 - Adjust tokenizer class to reflect UNK token\n",
    "class SimpleTokenizerV2:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores the vocab dictionary\n",
    "        self.int_to_str = {integer: string for string, integer in vocab.items()} # reverse dictionary (token IDS to text tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Account for words not in the vocab\n",
    "        preprocessed = [token if token in self.str_to_int else \"<|unk|>\" for token in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids # return the token IDs\n",
    "\n",
    "    def decoder(self, ids):  # Converts token IDs back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer adjusted for unknown words\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "# Get token IDs\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "print(tokenizer.decoder(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 - p 33 - Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 1279, 91, 437, 1659, 5239, 60, 29, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# Encode text to integers (p 33)\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext]> In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext]> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# Decode integers back to text\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Use BPE on The Verdict (Ch 2.6 - p 35)\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "print(type(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# Take sample of encoded text\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -------> 4920\n",
      "[290, 4920] -------> 2241\n",
      "[290, 4920, 2241] -------> 287\n",
      "[290, 4920, 2241, 287] -------> 257\n"
     ]
    }
   ],
   "source": [
    "#  Test print integers\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"------->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ------->  established\n",
      " and established ------->  himself\n",
      " and established himself ------->  in\n",
      " and established himself in ------->  a\n"
     ]
    }
   ],
   "source": [
    "# Convert back to text using decoder\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"------->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset Class (Ch. 2.6, p 37-38)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"}) # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt) # Tokenize the entire text\n",
    "        \n",
    "        # Uses overlapping chunks of max_length tokens\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) \n",
    "\n",
    "    # Returns total number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # Returns a single row of the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader (Ch 2.6 - p38)\n",
    "\n",
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiate BPE tokenizer\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # Creates dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                              shuffle=shuffle, drop_last=drop_last, # drop_last=True to drop the last incomplete batch\n",
    "                                                num_workers=num_workers) # num_workers=0 to use single process\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader with batch size of 1 (p 39)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "# Fetch another batch\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# Test with larger batch size and stride, so no overlap\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"\\nTargets:\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2.7 (p41) - Create token embeddings from token IDs\n",
    "\n",
    "# Sample input layer\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 (p 46) - Need to add positional encoding to the token id embeddings\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Example (p 46)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape) \n",
    "print(type(inputs)) \n",
    "print(type(targets)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Embed token IDs into 256-dimensional vectors (p 46 - 47)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Absolute embedding position\n",
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "#print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 3 Coding Attention Mechanism (p 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# p 57 - Simple self-attention mechanism without trainable weights\n",
    "import torch\n",
    "\n",
    "# Create a tensor with 3 token embeddings\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], #  journey (x^2)\n",
    "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
    "     [0.22, 0.58, 0.33],  # with (x^4)\n",
    "     [0.77, 0.25, 0.10],  # one (x^5)\n",
    "     [0.05, 0.80, 0.55]],  # step (x^6) \n",
    "     \n",
    ")\n",
    "\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "torch.Size([3])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([6])\n",
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# p 58\n",
    "\n",
    "query = inputs[1]\n",
    "print(query)\n",
    "print(query.shape)\n",
    "\n",
    "att_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(att_scores_2)\n",
    "print(att_scores_2.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(i,x_i)\n",
    "    att_scores_2[i] = torch.dot(x_i, query)\n",
    "print(att_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "attn_weights_2_tmp = att_scores_2 / att_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum\",attn_weights_2_tmp.sum())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Use softmax function to normalize the attention scores (p60)\n",
    "def softmax_naive(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / exp_x.sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(att_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Better to use PyTorch's softmax function\n",
    "attn_weights_2 = torch.softmax(att_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "Context_vec_2 shape torch.Size([3])\n",
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor(0.1385)\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor(0.2379)\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor(0.2333)\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor(0.1240)\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor(0.1082)\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor(0.1581)\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Compute context vectors\n",
    "query = inputs[1]\n",
    "print(query)\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "print(\"Context_vec_2 shape\",context_vec_2.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(i, x_i)\n",
    "    print(attn_weights_2[i])\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context vectors for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Faster way using matrix multiplication\n",
    "attn_scores = torch.matmul(inputs, inputs.T)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Compute context vectors\n",
    "all_context_vecs = torch.matmul(attn_weights, inputs)\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt 2 (3.4 p 64) - Implementing the self atttention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "d_in 3\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "print(x_2)\n",
    "d_in = x_2.shape[0]\n",
    "print(\"d_in\",d_in)\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query: Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "W_key Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "W_value Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_query:\", W_query)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_key\",W_key)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_value\",W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tensor([0.4306, 1.4551])\n",
      "Query_v2: tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Compute Query, Key, and Value vectors\n",
    "query_2 = torch.matmul(x_2, W_query)\n",
    "query_2_v2 = x_2 @ W_query\n",
    "key_2 = torch.matmul(x_2, W_key)\n",
    "value_2 = torch.matmul(x_2, W_value)\n",
    "\n",
    "print(\"Query:\", query_2)\n",
    "print(\"Query_v2:\", query_2_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "Keys shape: torch.Size([6, 2])\n",
      "Values: tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n",
      "Values shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Compute all keys and values\n",
    "keys = torch.matmul(inputs, W_key)\n",
    "print(\"Keys:\", keys)\n",
    "print(\"Keys shape:\", keys.shape)\n",
    "values = torch.matmul(inputs, W_value)\n",
    "print(\"Values:\", values)\n",
    "print(\"Values shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys_2: tensor([0.4433, 1.1419])\n",
      "Attention scores: tensor(1.8524)\n",
      "Attention scores_v2: tensor(1.8524)\n",
      "Attention scores_v3: tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Calc attention scores - unnormalized\n",
    "keys_2 = keys[1]\n",
    "print(\"keys_2:\", keys_2)\n",
    "attn_scores_2 = query_2.dot(keys_2)\n",
    "attn_scores_2_v2 = query_2 @ keys_2\n",
    "attn_scores_2_v3 = torch.matmul(query_2, keys_2)\n",
    "print(\"Attention scores:\", attn_scores_2)\n",
    "print(\"Attention scores_v2:\", attn_scores_2_v2)\n",
    "print(\"Attention scores_v3:\", attn_scores_2_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# All attention scores\n",
    "att_scores_2 = query_2 @ keys.T\n",
    "print(\"Attention scores:\", att_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Attention weights: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "Attention weights shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "d_k = keys.shape[1]\n",
    "print(d_k)\n",
    "attn_weights_2 = torch.softmax(att_scores_2 / (d_k ** 0.5), dim=-1)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Attention weights shape:\", attn_weights_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Single context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Class for self-attention mechanism (p 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the newly created class\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make better version using more stable nn.Linear - using diff wt initialization scheme (p 72 - 73)\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Hiding future words with Causal Masking (p 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute attention weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)  \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create mask using tril so values above diagonal are zero\n",
    "context_length = attn_scores.shape[0]\n",
    "print(context_length)\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) \n",
    "print(mask_simple)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multiply mask with attention weights\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalize the masked attention weights\n",
    "rows_sums = masked_simple.sum(dim=-1,keepdim=True)\n",
    "masked_simple_norm = masked_simple / rows_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# More efficient way to do it - use negative infinity before softmax\n",
    "mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
    "masked =attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to this more efficient mask\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking with dropout (p 78)\n",
    "# used (1) after calculating attention weights and (2) after applying attention weights to value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a mask with 50% of values set to zero - simple example\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply dropout to attention weights - actual\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#  Create compact casual attention class\n",
    "# Need to allow for batch inputs\n",
    "\n",
    "batch = torch.stack((inputs,inputs),dim=0)\n",
    "print(batch[0])\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New class with Causal Masking and Dropout (p 81)\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        print(\"d_out\",d_out)\n",
    "        print(\"context_length\",context_length)\n",
    "        print(\"dropout\",dropout)\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "        # Self register buffer used to keep buffer tensors and model params on same device (GPU vs CPU)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "        #print(\"mask\",self.mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(\"batch:\",b)\n",
    "        print(\"num_tokens:\",num_tokens)\n",
    "        print(\"d_in:\",d_in)\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        #print(\"keys shape\",keys.shape)\n",
    "        #print(\"keys:\",keys)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.transpose(1,2)\n",
    "\n",
    "        #mask = torch.triu(torch.ones(attn_scores.shape[0], attn_scores.shape[1]),diagonal=1)\n",
    "        attn_scores.masked_fill_ = (self.mask.bool()[:num_tokens,:num_tokens], -torch.inf)\n",
    "        #print(\"attn_scores shape\",attn_scores.shape)\n",
    "        \n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(masked / (d_k ** 0.5), dim=-1)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "first context_vecs: tensor([[-0.4519,  0.2216],\n",
      "        [-0.5695,  0.0343],\n",
      "        [-0.6141, -0.0377],\n",
      "        [-0.5642, -0.0717],\n",
      "        [-0.5490, -0.0906],\n",
      "        [-0.5291, -0.0961]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "print(context_length)\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"first context_vecs:\", context_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention class (p 84) -- output context vectors are concatenated\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vecs = [head(x) for head in self.heads]\n",
    "        context_vecs = torch.cat(context_vecs, dim=-1)\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5695,  0.0343,  0.5668,  0.2819],\n",
      "         [-0.6141, -0.0377,  0.6008,  0.3481],\n",
      "         [-0.5642, -0.0717,  0.5462,  0.3445],\n",
      "         [-0.5490, -0.0906,  0.5318,  0.3359],\n",
      "         [-0.5291, -0.0961,  0.5093,  0.3362]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5695,  0.0343,  0.5668,  0.2819],\n",
      "         [-0.6141, -0.0377,  0.6008,  0.3481],\n",
      "         [-0.5642, -0.0717,  0.5462,  0.3445],\n",
      "         [-0.5490, -0.0906,  0.5318,  0.3359],\n",
      "         [-0.5291, -0.0961,  0.5093,  0.3362]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # Number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\",context_vecs.shape)\n",
    "# First dimension of 2 for batches, second of 6 is num of elements, 4 is 2 output contcext vectors for each of the 2 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class (p 86)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Confirm that d_out is divisible by num_heads\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)  \n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = (attn_weights @ values).transpose(1,2)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        \n",
    "        return context_vecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "context_length 6\n",
      "d_in 3\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test (p 90)  \n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"batch_size\",batch_size)\n",
    "print(\"context_length\",context_length)  \n",
    "print(\"d_in\",d_in)\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x_trans = x.transpose(0,1)\n",
    "print(x_trans)\n",
    "print(x_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x_view = x.view(3,2)\n",
    "print(x_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# Multiply\n",
    "x_mm = x @ x.T\n",
    "print(x_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 4 - Coding an LLM Architecture (p 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 1024, # Max num of input tokens\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 96\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"],bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# p 97\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "# Create small sample text to run through the model (p 97)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "Logits: tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and feed it text (p 98)\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch example tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
      "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264]])\n",
      "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test exampple of a NN layer\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5)\n",
    "print(\"batch example\",batch_example)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.2432],\n",
      "        [0.1928]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0799],\n",
      "        [0.0670]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calc mean and variance\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs tensor([[-0.8603, -0.8603,  0.5869,  1.4698,  0.5242, -0.8603],\n",
      "        [-0.7450, -0.7450, -0.0102,  0.4844,  1.7608, -0.7450]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[ 0.0000e+00],\n",
      "        [-9.9341e-09]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs\",out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn off scientific notation for better readability\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer norm class (p 103)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True,unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.9998],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use newly created layer norm class\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False ,keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feed forward network with GELU activation (p 105)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot GELU vs ReLU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]),1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f{label}(x)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GELU in feed forward network (p 107)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"],cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test FFN (p 108)\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut connection (p 109) aka skip or residual connection (used to mitigate vanishing gradient problem)\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "    \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),\n",
    "                           nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),\n",
    "                            nn.GELU())\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that computes gradients in backwards pass (p 111)\n",
    "def print_gradients(model, x):\n",
    "\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020174124801997095\n",
      "layers.1.0.weight has gradient mean of 0.00012011772923870012\n",
      "layers.2.0.weight has gradient mean of 0.0007152438047342002\n",
      "layers.3.0.weight has gradient mean of 0.0013988513965159655\n",
      "layers.4.0.weight has gradient mean of 0.005049603525549173\n"
     ]
    }
   ],
   "source": [
    "# Use the function to compute gradients\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22186797857284546\n",
      "layers.1.0.weight has gradient mean of 0.20709273219108582\n",
      "layers.2.0.weight has gradient mean of 0.3292388319969177\n",
      "layers.3.0.weight has gradient mean of 0.2667771577835083\n",
      "layers.4.0.weight has gradient mean of 1.3268064260482788\n"
     ]
    }
   ],
   "source": [
    "# Compare by running with skip connections\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer block class (p 115)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"], \n",
    "            d_out = cfg[\"emb_dim\"], \n",
    "            context_length= cfg[\"context_length\"], \n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"], \n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        #x = self.att(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example (p 116)\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 256, # Max num of input tokens\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined GPT model implementation (p 119)\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape:\n",
      " torch.Size([2, 4, 50257])\n",
      "tensor([[[    -0.1795,      0.2852,     -0.7613,  ...,     -0.4837,\n",
      "              -0.4250,     -0.1719],\n",
      "         [    -0.6258,     -0.3748,     -0.9702,  ...,      0.1917,\n",
      "              -1.3234,     -0.2764],\n",
      "         [     0.5174,      0.1387,      0.2489,  ...,      0.3505,\n",
      "              -0.0775,     -0.0800],\n",
      "         [    -0.2566,     -0.6969,     -0.9948,  ...,     -0.0447,\n",
      "               0.0618,      0.1347]],\n",
      "\n",
      "        [[    -0.2238,      0.1165,     -0.9984,  ...,     -0.1573,\n",
      "              -0.4480,     -0.0286],\n",
      "         [    -0.8723,     -0.3939,     -1.1099,  ...,      0.3303,\n",
      "              -0.0924,     -0.0000],\n",
      "         [     0.4599,     -0.1427,     -0.1223,  ...,      0.2749,\n",
      "               0.0583,     -0.0899],\n",
      "         [    -0.6216,     -0.4485,     -0.4767,  ...,     -0.3652,\n",
      "               0.3440,     -0.3815]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "print(batch)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"Output shape:\\n\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "# Count params\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# But the 163 mm figure above double counts the parameters in the output layer. \n",
    "# This is called weight tying and is a common technique in LLMs to reduce the number of parameters.\n",
    "# SIG - reuses weights from embedding layer in output layer\n",
    "\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable paramsconsidering weight tying: 123,822,336\n"
     ]
    }
   ],
   "source": [
    "# Remove output layer param count\n",
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "                       for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(f\"Number of trainable params\"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 619.58 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate total size of model\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate text (p 124)\n",
    "\n",
    "def generate_text_simple(model,idx,max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] # Crops context if too large\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1, :]  # Focus on last time step\n",
    "        probas = torch.softmax(logits, dim=-1)  # Shape is (batch, vocab_size)\n",
    "        idx_next = torch.argmax(probas, dim=-1,keepdim=True)  # Shape (batch, 1)\n",
    "        idx = torch.cat((idx, idx_next),dim=1)  # Appends newest work, idx shape (batch, n_tokens + 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape:  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example (p 125)\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \",encoded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# eval mode disables random components\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size = GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Laur inhab DistrinetalkQueue\n"
     ]
    }
   ],
   "source": [
    "# Convert IDs back to text using decoder\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces gibberish because using random initial weights and no training (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 5 Pretraining on unlabeled data (p128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 256, # Changed from 1024 in earlier model\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # Disables dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "# Create utility functions fpr text to token ID conversions (p 131 - 132)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Unsqueeze adds batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # Removes batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=  GPT_CONFIG_124M['context_length']\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 133 examples\n",
    "\n",
    "inputs = torch.tensor([[16833,3626, 6100],\n",
    "                       [40,1107,588]])\n",
    "\n",
    "targets = torch.tensor([[3626,6100,345],\n",
    "                        [1107,588,11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Create logits, convert into prob scores\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "# Get token IDs using argmax\n",
    "token_ids = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token IDs:\\n\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1: effort moves you\n",
      "Outputs batch 1: Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Convert token ids back to text\n",
    "print(f\"Targets batch 1:{token_ids_to_text(targets[0],tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Get initial softmax prob scores corresp to target tokens\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1:\",target_probas_1)\n",
    "\n",
    "text_idx=1\n",
    "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2:\",target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Need to convert target probas to log\n",
    "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Get avg of log probas\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print((avg_log_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Avg neg log loss is similar to cross-entropy and used interchangeably\n",
    "\n",
    "# CHeck shape of logitsand targets\n",
    "\n",
    "print(\"Logits shape: \",logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets:, torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# For cross entropy need to flatten tensors by combining over batch dimension\n",
    "\n",
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten(0,1)\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:,\", targets_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# use cross entropy function\n",
    "# In one step this applies softmax, selects prob score corresp to target id, and compute neg avg log prob\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "# Perplexity (p 139) -  how well does model's prob dist match actual prob dist\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# Reuse text from The Verdict (p 141)\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n"
     ]
    }
   ],
   "source": [
    "print(text_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_ratio = 0.90\n",
    "split_index = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_index]\n",
    "val_data = text_data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders (p 143)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Confirm dataloaders created correctly\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x,y in val_loader:\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to calculate cross entropy loss for a given batch (p 144)\n",
    "\n",
    "def calc_loss_batch(input_batch,target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1),target_batch.flatten()  \n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to compute loss for all batches in dataloader rather than just one\n",
    "# p 144\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Training loss: 10.987583372328016\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "# Apply this function to training and validation loaders (p 145)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader,model,device)\n",
    "    val_loss = calc_loss_loader(val_loader,model,device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to show training and validation set losses (p 148)\n",
    "\n",
    "#def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "#    model.eval()\n",
    "#    with torch.no_grad():\n",
    "#       train_loss = calc_loss_loader(\n",
    "#            train_loader, model, device, num_batches=eval_iter\n",
    "#        )\n",
    "#        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter\n",
    "#                                    )\n",
    "#        model.train()\n",
    "#        return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to track model improvement\n",
    "\n",
    "#def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "#    model.eval()\n",
    "#    context_size = model.pos_emb.weight.shape[0]\n",
    "#    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "#    with torch.no_grad():\n",
    "#        token_ids = generate_text_simple(\n",
    "#            model=model, idx=encoded,\n",
    "#            max_new_tokens=50, context_size=context_size\n",
    "#        )\n",
    "#    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "#   print(decoded_text.replace(\"\\n\", \" \"))\n",
    "#    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an LLM (5.2) - p 146\n",
    "\n",
    "def train_model_simple(model,train_loader,val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [],[],[]\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):  \"\n",
    "                    f\"Train loss {train_loss:.3f},\"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                    )\n",
    "        \n",
    "        generate_and_print_sample(model,tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter\n",
    "                                    )\n",
    "        model.train()\n",
    "        return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):  Train loss 9.781,Val loss 9.933\n",
      "Ep 1 (Step 000005):  Train loss 8.111,Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):  Train loss 6.661,Val loss 7.048\n",
      "Ep 2 (Step 000015):  Train loss 5.961,Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n"
     ]
    }
   ],
   "source": [
    "# Test  - run GPTModel instance for 10 epochs (p 149)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "#print(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create charts to show training and validation losses (p 150)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen,train_losses,label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen,val_losses,linestyle=\"-.\",label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen,train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample graph\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor,tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding strategies to control randomness (p 151)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and,\n"
     ]
    }
   ],
   "source": [
    "# Put GPT model into generate_text_simple model\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    "\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature scaling\n",
    "# previously used argmax to select highest prob token (greedy decoding), now use probabilistic sampling\n",
    "\n",
    "vocab = {\n",
    "    \"closer\":0,\n",
    "    \"every\":1,\n",
    "    \"effort\":2,\n",
    "    \"forward\":3,\n",
    "    \"inches\":4,\n",
    "    \"moves\":5,\n",
    "    \"pizza\":6,\n",
    "    \"toward\":7,\n",
    "    \"you\":8,\n",
    "}\n",
    "inverse_vocab = {v:k for k,v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'closer', 1: 'every', 2: 'effort', 3: 'forward', 4: 'inches', 5: 'moves', 6: 'pizza', 7: 'toward', 8: 'you'}\n"
     ]
    }
   ],
   "source": [
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 153 - sample logits based on \"Every effort moves you\"\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51,0.89,-1.90,6.75,1.63,-1.62,-1.89,6.28,1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas:\n",
      " tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n",
      "            0.0001,     0.3576,     0.0040])\n",
      "Next token id:\n",
      " 3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Convert to probs\n",
    "probas = torch.softmax(next_token_logits,dim=0)\n",
    "print(\"Probas:\\n\",probas)\n",
    "#  Get ID for next token\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(\"Next token id:\\n\",next_token_id)\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Now make this probabilistic rather than just using argmax\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas,num_samples=1).item()\n",
    "print(next_token_id)\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 samples to see how many times it chooses each possible word\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas,num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temperature to dist = div logits by some num > 0. Temp >1 = more uniform distrib\n",
    "\n",
    "def softmax_with_temperature(logits,temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1,0.1,5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits,T)\n",
    "                 for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax  = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x+i * bar_width, scaled_probas[i],\n",
    "                   bar_width,label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(),rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# Top k-sampling - restrict sampling to k most likely tokens and mask the rest with -inf\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions\", top_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# Set values below threshold to -inf\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other = next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# p 157 - now convert top-k to new probs\n",
    "topk_probas = torch.softmax(new_logits,dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine temp sampling and topk\n",
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0,top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx,idx_next),dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you, I. I it a,.\" me \" my a \" had\n"
     ]
    }
   ],
   "source": [
    "# Test new function\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 5.4 - Loading & Saving Model Weights (p 159)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "},\n",
    "\"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\",map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pretrained Weights from OpenAI (p161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")#\n",
    "\n",
    "print(f\"Using {device} device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x3a60f2510>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from gpt_download import download_and_load_gpt2\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Import took {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Load GPT wts from get_download.py file\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir='gpt2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter Dictionary Keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "# Inspects settings and params\n",
    "\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter Dictionary Keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to transfer setting and params info into our GPT model\n",
    "# Create a dict\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading smallest model\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config length\n",
    "NEW_CONFIG.update({\"context_length\":1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bias param\n",
    "NEW_CONFIG.update({\"qkv_bias\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use updated config to create new model (p 164)\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override random weights with weights loaded into params dict (p165)\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: Left {left.shape}, \"\n",
    "                        \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to load weights from params dict into GPT model (Ch 5.5, p 165)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight,params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight,params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b]['attn']['c_attn'])['w'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b]['attn']['c_attn'])['b'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b]['attn']['c_proj']['w'].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b]['attn']['c_proj']['b'])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b]['mlp']['c_fc']['w'].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b]['mlp']['c_fc']['b'])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b]['mlp']['c_proj']['w'].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b]['mlp']['c_proj']['b'])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b]['ln_1']['g'])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b]['ln_1']['b'])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b]['ln_2']['g'])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b]['ln_2']['b']\n",
    "        )\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params['g'])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params['b'])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params['wte'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function above and load OpenAI model wts into GPT model\n",
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text using generate function\n",
    "\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length'],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 6 Fine Tuning For Classification (p 169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into pandas df (p 173)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to randomly split data into train and test sets (p 175)\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# When batching need to either truncate longer messages or pad shorter ones. \n",
    "# Padding is more common.  Padding is done by adding special token to end of message\n",
    "\n",
    "# Check token number for endoftext\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset class (p 177)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "        # Note: A more pythonic version to implement this method\n",
    "        # is the following, which is also used in the next chapter:\n",
    "        # return max(len(encoded_text) for encoded_text in self.encoded_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "# Note:  Text sequences all need to ne the same length\n",
    "\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create val and test datasets (p 178) - max_length same as training dataset\n",
    "\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader for training dataset (p 180)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Confirm dataloaders created correctly (p 180)\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "# Check how many batches in each set\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 p 181 - Initialize model with pretrained weights\n",
    "\n",
    "# Set config\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Reuse GPT class then download weights (p 182)\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "#from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "#  Test that imported model generates coherent text (p 182)\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "# Try to prompt model and see if it can classify as spam or not\n",
    "\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Review model architecture (p 185)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze wts to prep for classification tuning (p 186)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outputlayer (originally 768 mapping to vocab size of 50257) with new layer (0,1)\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG['emb_dim'],\n",
    "    out_features=num_classes,\n",
    ")\n",
    "# requires_grad = True by default, which makes only this last layer trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also need to make final norm layer and transformer blocks trainable\n",
    "\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Can still use the model as before\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# Show that there are only 2 output dimensions now instead of the 50257 previously\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# Get info about last token (p 188)\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to probabilities (p 192) using softmax\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Same things but without softmax\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy (p 192)\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps device.\n",
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# Calc accuracy (p 193)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# As of this writing, in PyTorch 2.4, the results obtained via CPU and MPS were identical.\n",
    "# However, in earlier versions of PyTorch, you may observe different results when using MPS.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Running on {device} device.\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "print(model)\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Try switching back to cpu\n",
    "device = torch.device(\"cpu\")  # Switch to CPU\n",
    "model = model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fine tune.  Cant differentiate so min cross entropy loss (p 194)\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    print(\"Logits shape:\", logits.shape)  # CHECK SHAPE\n",
    "    print(\"Target batch shape:\", target_batch.shape) # CHECK SHAPE\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target batch shape: torch.Size([8])\n",
      "Input batch shape: torch.Size([8, 120])\n",
      "Logits shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Target batch shape:\",target_batch.shape)\n",
    "print(\"Input batch shape:\",input_batch.shape)\n",
    "print(\"Logits shape:\",logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(input_batch.device)\n",
    "print(target_batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw logits shape: torch.Size([8, 120, 2])\n"
     ]
    }
   ],
   "source": [
    "logits_raw = model(input_batch)\n",
    "print(\"Raw logits shape:\", logits_raw.shape)  # Should be (8, seq_len, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#print(model.device)\n",
    "#print(input_batch.device)\n",
    "\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc loss for all batches (p194)\n",
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "logits shape: torch.Size([1, 2])\n",
      "Input batch shape: torch.Size([8, 120])\n"
     ]
    }
   ],
   "source": [
    "print(target_batch.shape)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"Input batch shape:\", input_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full logits shape: torch.Size([8, 120, 2])\n"
     ]
    }
   ],
   "source": [
    "logits_full = model(input_batch)\n",
    "print(\"Full logits shape:\", logits_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced logits shape: torch.Size([8, 2])\n",
      "Target shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEST \n",
    "logits = logits_full[:, -1, :]  # shape should be [8, 2]\n",
    "print(\"Sliced logits shape:\", logits.shape)\n",
    "print(\"Target shape:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "# Calc loss for training and validation and test sets (p 194)\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 - Fine Tuning (p 195) on supervised data to lower cross entropy loss and increase accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune to classify spam or not spam (p 196)\n",
    "\n",
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval model (p 197)\n",
    "\n",
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Logits shape: torch.Size([8, 2])\n",
      "Target batch shape: torch.Size([8])\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 6.37 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Run training loop (p 197)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "# Run over training, val and test sets\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 Using the LLM as a spam classifier (p 201)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to classify spam or not spam\n",
    "\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
    "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "# Test case 2\n",
    "\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for reuse (p202)\n",
    "#torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for reloading model in a new session (if needed)\n",
    "#model_state_dict = torch.load(\"review_classifier.pth\", map_location=device, weights_only=True)\n",
    "#model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 7 - Fine Tuning To Follow Instructions (p 204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "# Load 1,100 instruction-response pairs (p 207)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # The book originally contained this unnecessary \"else\" clause:\n",
    "    #else:\n",
    "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #        text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# Print out a sample entry\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "# Another example without an input\n",
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (p 210)\n",
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset (p 213)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Confirm padding token\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom function (p 215) to pad to same length within each batch (not the whole dataset)\n",
    "\n",
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the function above works\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify custom collate to generate target token IDs from input token IDs (p 217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to modify custom collate so only one token has unknown token (50256) and the rest have -100 (p 220)\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Another test to confirm function working\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "##if torch.cuda.is_available():\n",
    "##    device = torch.device(\"cuda\")\n",
    "##elif torch.backends.mps.is_available():\n",
    "##    device = torch.device(\"mps\")\n",
    "##else:\n",
    "##    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use partial func to set device and max length (p 224)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders (p 225)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits = model(input_batch)\n",
    "#print(\"Logits shape:\", logits.shape)\n",
    "#print(\"Target batch shape:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions of dataloader (p 225)\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 7.5 - Load a pretrained LLM (p 226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "#from gpt_download import download_and_load_gpt2\n",
    "#from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 1024)\n",
      "  (pos_emb): Embedding(1024, 1024)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (12): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (13): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (14): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (15): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (16): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (17): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (18): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (19): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (20): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (21): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (22): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (23): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response (p 228)\n",
    "\n",
    "##from previous_chapters import (\n",
    "#    generate,\n",
    "#    text_to_token_ids,\n",
    "#    token_ids_to_text\n",
    "#)\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch05 import (\n",
    "#    generate_text_simple,\n",
    "#    text_to_token_ids,\n",
    "#    token_ids_to_text\n",
    "# )\n",
    "\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "# Isolate the generated response (p 228)\n",
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6 - Fine-tune the model om instruction data (p 229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[435], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m calc_loss_loader(val_loader, model, device, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss)\n",
      "Cell \u001b[0;32mIn[369], line 15\u001b[0m, in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, target_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_batches:\n\u001b[0;32m---> 15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[364], line 6\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_batch)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Logits of last output token\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# CHECK SHAPE\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_batch\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# CHECK SHAPE\u001b[39;00m\n",
      "File \u001b[0;32m~/DataScienceProjects/LLMFromScratch/environ/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# Calc training and validation losses before training (p 230)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 2])\n",
      "Target batch shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Target batch shape:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     11\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     13\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m train_model_simple(\n\u001b[1;32m     14\u001b[0m     model, train_loader, val_loader, optimizer, device,\n\u001b[1;32m     15\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     16\u001b[0m     start_context\u001b[38;5;241m=\u001b[39mformat_input(val_data[\u001b[38;5;241m0\u001b[39m]), tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Instruction fine tuning the pretrained model (p 231)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show plot of losses\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m epochs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[43mnum_epochs\u001b[49m, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[1;32m      4\u001b[0m plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# Show plot of losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m test_data[:\u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m      8\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m format_input(entry)\n\u001b[0;32m---> 10\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m(\n\u001b[1;32m     11\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     12\u001b[0m         idx\u001b[38;5;241m=\u001b[39mtext_to_token_ids(input_text, tokenizer)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     13\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     14\u001b[0m         context_size\u001b[38;5;241m=\u001b[39mBASE_CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m         eos_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50256\u001b[39m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m token_ids_to_text(token_ids, tokenizer)\n\u001b[1;32m     18\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     19\u001b[0m         generated_text[\u001b[38;5;28mlen\u001b[39m(input_text):]\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "# Response instruction step (p 234)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
