{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 20479\n",
      "I HAD alw\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and review the first 100 characters\n",
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Length of text:\", len(raw_text))\n",
    "print(raw_text[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3]\n",
    "tens1 = torch.tensor(data)\n",
    "print(tens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "array = np.array([1,2,3])\n",
    "print(array)\n",
    "tens2 = torch.from_numpy(array)\n",
    "print(tens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ch 2, p 22 - split text\n",
    "import re\n",
    "text = \"Hello, world.  This, is a test.\"\n",
    "result = re.split(r'([!,.]|\\s)', text)\n",
    "print(result)\n",
    "result2 = [item for item in result if item.strip()]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Add more puncuation and  --- to the split (p 23)\n",
    "text = \"Hello, world.  Is this-- a test?\"\n",
    "result = re.split(r'([!,.?:]|--|\\s)', text)\n",
    "print(result)\n",
    "result2 = [item for item in result if item.strip()]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "# p24 - Use tokenizer scheme on Wharton text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Test output of tokenized used on The Verdict  \n",
    "print(preprocessed[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "['yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# 2.3 - Create a vocabulary (p 25)\n",
    "\n",
    "# Create list of all uniuqe words and sort\n",
    "all_words   = sorted(set(preprocessed))\n",
    "vocab_size  = len(all_words)\n",
    "print(vocab_size)\n",
    "print(all_words[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Use vocab to create a dictionary\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 27 - Make simple tokenizer Class\n",
    "class SimpleTokenV1:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores the vocab dictionary\n",
    "        self.int_to_str = {integer: string for string, integer in vocab.items()} # reverse dictionary (token IDS to text tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids # return the token IDs\n",
    "\n",
    "    def decoder(self, ids):  # Converts token IDs back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "tokenizer = SimpleTokenV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# Need to account for words not in the vocab (p 30)\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that new characters are in the vocab\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 31 - Adjust tokenizer class to reflect UNK token\n",
    "class SimpleTokenizerV2:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores the vocab dictionary\n",
    "        self.int_to_str = {integer: string for string, integer in vocab.items()} # reverse dictionary (token IDS to text tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Account for words not in the vocab\n",
    "        preprocessed = [token if token in self.str_to_int else \"<|unk|>\" for token in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids # return the token IDs\n",
    "\n",
    "    def decoder(self, ids):  # Converts token IDs back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer adjusted for unknown words\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "# Get token IDs\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "print(tokenizer.decoder(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 - p 33 - Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 1279, 91, 437, 1659, 5239, 60, 29, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# Encode text to integers (p 33)\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext]> In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext]> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# Decode integers back to text\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Use BPE on The Verdict (Ch 2.6 - p 35)\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "print(type(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# Take sample of encoded text\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -------> 4920\n",
      "[290, 4920] -------> 2241\n",
      "[290, 4920, 2241] -------> 287\n",
      "[290, 4920, 2241, 287] -------> 257\n"
     ]
    }
   ],
   "source": [
    "#  Test print integers\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"------->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ------->  established\n",
      " and established ------->  himself\n",
      " and established himself ------->  in\n",
      " and established himself in ------->  a\n"
     ]
    }
   ],
   "source": [
    "# Convert back to text using decoder\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"------->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset Class (Ch. 2.6, p 37-38)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"}) # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt) # Tokenize the entire text\n",
    "        \n",
    "        # Uses overlapping chunks of max_length tokens\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) \n",
    "\n",
    "    # Returns total number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # Returns a single row of the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader (Ch 2.6 - p38)\n",
    "\n",
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiate BPE tokenizer\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # Creates dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                              shuffle=shuffle, drop_last=drop_last, # drop_last=True to drop the last incomplete batch\n",
    "                                                num_workers=num_workers) # num_workers=0 to use single process\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader with batch size of 1 (p 39)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "# Fetch another batch\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# Test with larger batch size and stride, so no overlap\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"\\nTargets:\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2.7 (p41) - Create token embeddings from token IDs\n",
    "\n",
    "# Sample input layer\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 (p 46) - Need to add positional encoding to the token id embeddings\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Example (p 46)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape) \n",
    "print(type(inputs)) \n",
    "print(type(targets)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Embed token IDs into 256-dimensional vectors (p 46 - 47)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[-0.0923, -0.3579, -1.4693,  ...,  0.8824,  0.5271, -0.7098],\n",
      "        [-1.3212, -0.2870, -0.3833,  ..., -0.6094,  0.2991, -0.1256],\n",
      "        [-0.4608,  0.9199,  0.3221,  ..., -0.4178, -1.7754,  0.4771],\n",
      "        [-1.4604,  0.7514,  0.1482,  ..., -1.9828,  0.5930,  0.5207]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Absolute embedding position\n",
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "#print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 3 Coding Attention Mechanism (p 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# p57 - Simple self-attention mechanism without trainable weights\n",
    "import torch\n",
    "\n",
    "# Create a tensor with 3 token embeddings\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], #  journey (x^2)\n",
    "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
    "     [0.22, 0.58, 0.33],  # with (x^4)\n",
    "     [0.77, 0.25, 0.10],  # one (x^5)\n",
    "     [0.05, 0.80, 0.55]],  # step (x^6) \n",
    "     \n",
    ")\n",
    "\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "torch.Size([3])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([6])\n",
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# p 58\n",
    "\n",
    "query = inputs[1]\n",
    "print(query)\n",
    "print(query.shape)\n",
    "\n",
    "att_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(att_scores_2)\n",
    "print(att_scores_2.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(i,x_i)\n",
    "    att_scores_2[i] = torch.dot(x_i, query)\n",
    "print(att_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "attn_weights_2_tmp = att_scores_2 / att_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum\",attn_weights_2_tmp.sum())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Use softmax function to normalize the attention scores (p60)\n",
    "def softmax_naive(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / exp_x.sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(att_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Better to use PyTorch's softmax function\n",
    "attn_weights_2 = torch.softmax(att_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "Context_vec_2 shape torch.Size([3])\n",
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor(0.1385)\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor(0.2379)\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor(0.2333)\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor(0.1240)\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor(0.1082)\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor(0.1581)\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Compute context vectors\n",
    "query = inputs[1]\n",
    "print(query)\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "print(\"Context_vec_2 shape\",context_vec_2.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(i, x_i)\n",
    "    print(attn_weights_2[i])\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context vectors for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Faster way using matrix multiplication\n",
    "attn_scores = torch.matmul(inputs, inputs.T)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Compute context vectors\n",
    "all_context_vecs = torch.matmul(attn_weights, inputs)\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt 2 (3.4 p 64) - Implementing the self atttention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "d_in 3\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "print(x_2)\n",
    "d_in = x_2.shape[0]\n",
    "print(\"d_in\",d_in)\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query: Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "W_key Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "W_value Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_query:\", W_query)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_key\",W_key)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_value\",W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tensor([0.4306, 1.4551])\n",
      "Query_v2: tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Compute Query, Key, and Value vectors\n",
    "query_2 = torch.matmul(x_2, W_query)\n",
    "query_2_v2 = x_2 @ W_query\n",
    "key_2 = torch.matmul(x_2, W_key)\n",
    "value_2 = torch.matmul(x_2, W_value)\n",
    "\n",
    "print(\"Query:\", query_2)\n",
    "print(\"Query_v2:\", query_2_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "Keys shape: torch.Size([6, 2])\n",
      "Values: tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n",
      "Values shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Compute all keys and values\n",
    "keys = torch.matmul(inputs, W_key)\n",
    "print(\"Keys:\", keys)\n",
    "print(\"Keys shape:\", keys.shape)\n",
    "values = torch.matmul(inputs, W_value)\n",
    "print(\"Values:\", values)\n",
    "print(\"Values shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys_2: tensor([0.4433, 1.1419])\n",
      "Attention scores: tensor(1.8524)\n",
      "Attention scores_v2: tensor(1.8524)\n",
      "Attention scores_v3: tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Calc attention scores - unnormalized\n",
    "keys_2 = keys[1]\n",
    "print(\"keys_2:\", keys_2)\n",
    "attn_scores_2 = query_2.dot(keys_2)\n",
    "attn_scores_2_v2 = query_2 @ keys_2\n",
    "attn_scores_2_v3 = torch.matmul(query_2, keys_2)\n",
    "print(\"Attention scores:\", attn_scores_2)\n",
    "print(\"Attention scores_v2:\", attn_scores_2_v2)\n",
    "print(\"Attention scores_v3:\", attn_scores_2_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# All attention scores\n",
    "att_scores_2 = query_2 @ keys.T\n",
    "print(\"Attention scores:\", att_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Attention weights: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "Attention weights shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "d_k = keys.shape[1]\n",
    "print(d_k)\n",
    "attn_weights_2 = torch.softmax(att_scores_2 / (d_k ** 0.5), dim=-1)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Attention weights shape:\", attn_weights_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Single context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Class for self-attention mechanism (p 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the newly created class\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make better version using more stable nn.Linear - using diff wt initialization scheme (p 72 - 73)\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Hiding future words with Causal Masking (p 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute attention weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)  \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create mask using tril so values above diagonal are zero\n",
    "context_length = attn_scores.shape[0]\n",
    "print(context_length)\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) \n",
    "print(mask_simple)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multiply mask with attention weights\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalize the masked attention weights\n",
    "rows_sums = masked_simple.sum(dim=-1,keepdim=True)\n",
    "masked_simple_norm = masked_simple / rows_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# More efficient way to do it - use negative infinity before softmax\n",
    "mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
    "masked =attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to this more efficient mask\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking with dropout (p 78)\n",
    "# used (1) after calculating attention weights and (2) after applying attention weights to value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a mask with 50% of values set to zero - simple example\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply dropout to attention weights - actual\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#  Create compact casual attention class\n",
    "# Need to allow for batch inputs\n",
    "\n",
    "batch = torch.stack((inputs,inputs),dim=0)\n",
    "print(batch[0])\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New class with Causal Masking and Dropout (p 81)\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        print(\"d_out\",d_out)\n",
    "        print(\"context_length\",context_length)\n",
    "        print(\"dropout\",dropout)\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "        # Self register buffer used to keep buffer tensors and model params on same device (GPU vs CPU)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "        #print(\"mask\",self.mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(\"batch:\",b)\n",
    "        print(\"num_tokens:\",num_tokens)\n",
    "        print(\"d_in:\",d_in)\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        #print(\"keys shape\",keys.shape)\n",
    "        #print(\"keys:\",keys)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.transpose(1,2)\n",
    "\n",
    "        #mask = torch.triu(torch.ones(attn_scores.shape[0], attn_scores.shape[1]),diagonal=1)\n",
    "        attn_scores.masked_fill_ = (self.mask.bool()[:num_tokens,:num_tokens], -torch.inf)\n",
    "        #print(\"attn_scores shape\",attn_scores.shape)\n",
    "        \n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(masked / (d_k ** 0.5), dim=-1)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "first context_vecs: tensor([[-0.4519,  0.2216],\n",
      "        [-0.5695,  0.0343],\n",
      "        [-0.6141, -0.0377],\n",
      "        [-0.5642, -0.0717],\n",
      "        [-0.5490, -0.0906],\n",
      "        [-0.5291, -0.0961]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "print(context_length)\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"first context_vecs:\", context_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention class (p 84) -- output context vectors are concatenated\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vecs = [head(x) for head in self.heads]\n",
    "        context_vecs = torch.cat(context_vecs, dim=-1)\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5695,  0.0343,  0.5668,  0.2819],\n",
      "         [-0.6141, -0.0377,  0.6008,  0.3481],\n",
      "         [-0.5642, -0.0717,  0.5462,  0.3445],\n",
      "         [-0.5490, -0.0906,  0.5318,  0.3359],\n",
      "         [-0.5291, -0.0961,  0.5093,  0.3362]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5695,  0.0343,  0.5668,  0.2819],\n",
      "         [-0.6141, -0.0377,  0.6008,  0.3481],\n",
      "         [-0.5642, -0.0717,  0.5462,  0.3445],\n",
      "         [-0.5490, -0.0906,  0.5318,  0.3359],\n",
      "         [-0.5291, -0.0961,  0.5093,  0.3362]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # Number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\",context_vecs.shape)\n",
    "# First dimension of 2 for batches, second of 6 is num of elements, 4 is 2 output contcext vectors for each of the 2 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class (p 86)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Confirm that d_out is divisible by num_heads\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)  \n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = (attn_weights @ values).transpose(1,2)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        \n",
    "        return context_vecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "context_length 6\n",
      "d_in 3\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test (p 90)  \n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"batch_size\",batch_size)\n",
    "print(\"context_length\",context_length)  \n",
    "print(\"d_in\",d_in)\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x_trans = x.transpose(0,1)\n",
    "print(x_trans)\n",
    "print(x_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x_view = x.view(3,2)\n",
    "print(x_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# Multiply\n",
    "x_mm = x @ x.T\n",
    "print(x_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 4 - Coding an LLM Architecture (p 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 1024, # Max num of input tokens\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 96\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"],bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# p 97\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "# Create small sample text to run through the model (p 97)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "Logits: tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0448,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and feed it text (p 98)\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch example tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
      "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264]])\n",
      "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test exampple of a NN layer\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5)\n",
    "print(\"batch example\",batch_example)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.2432],\n",
      "        [0.1928]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0799],\n",
      "        [0.0670]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calc mean and variance\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs tensor([[-0.8603, -0.8603,  0.5869,  1.4698,  0.5242, -0.8603],\n",
      "        [-0.7450, -0.7450, -0.0102,  0.4844,  1.7608, -0.7450]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[ 0.0000e+00],\n",
      "        [-4.4703e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs\",out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn off scientific notation for better readability\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer norm class (p 103)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True,unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.9998],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use newly created layer norm class\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False ,keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feed forward network with GELU activation (p 105)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot GELU vs ReLU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]),1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f{label}(x)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GELU in feed forward network (p 107)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"],cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test FFN (p 108)\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut connection (p 109) aka skip or residual connection (used to mitigate vanishing gradient problem)\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "    \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),\n",
    "                           nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),\n",
    "                            nn.GELU())\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that computes gradients in backwards pass (p 111)\n",
    "def print_gradients(model, x):\n",
    "\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0002017411752603948\n",
      "layers.1.0.weight has gradient mean of 0.00012011770741082728\n",
      "layers.2.0.weight has gradient mean of 0.0007152436301112175\n",
      "layers.3.0.weight has gradient mean of 0.0013988513965159655\n",
      "layers.4.0.weight has gradient mean of 0.005049605388194323\n"
     ]
    }
   ],
   "source": [
    "# Use the function to compute gradients\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00437004491686821\n",
      "layers.1.0.weight has gradient mean of 0.0026738042943179607\n",
      "layers.2.0.weight has gradient mean of 0.007288849912583828\n",
      "layers.3.0.weight has gradient mean of 0.006180735770612955\n",
      "layers.4.0.weight has gradient mean of 0.018918858841061592\n"
     ]
    }
   ],
   "source": [
    "# Compare by running with skip connections\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer block class (p 115)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"], \n",
    "            d_out = cfg[\"emb_dim\"], \n",
    "            context_length= cfg[\"context_length\"], \n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"], \n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        #x = self.att(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example (p 116)\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 256, # Max num of input tokens\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined GPT model implementation (p 119)\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape:\n",
      " torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "print(batch)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"Output shape:\\n\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "# Count params\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# But the 163 mm figure above double counts the parameters in the output layer. \n",
    "# This is called weight tying and is a common technique in LLMs to reduce the number of parameters.\n",
    "# SIG - reuses weights from embedding layer in output layer\n",
    "\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable paramsconsidering weight tying: 123,822,336\n"
     ]
    }
   ],
   "source": [
    "# Remove output layer param count\n",
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "                       for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(f\"Number of trainable params\"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 619.58 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate total size of model\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate text (p 124)\n",
    "\n",
    "def generate_text_simple(model,idx,max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] # Crops context if too large\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1, :]  # Focus on last time step\n",
    "        probas = torch.softmax(logits, dim=-1)  # Shape is (batch, vocab_size)\n",
    "        idx_next = torch.argmax(probas, dim=-1,keepdim=True)  # Shape (batch, 1)\n",
    "        idx = torch.cat((idx, idx_next),dim=1)  # Appends newest work, idx shape (batch, n_tokens + 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape:  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example (p 125)\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \",encoded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# eval mode disables random components\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size = GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "# Convert IDs back to text using decoder\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces gibberish because using random initial weights and no training (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 5 Pretraining on unlabeled data (p128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 256, # Changed from 1024 in earlier model\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # Disables dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "# Create utility functions fpr text to token ID conversions (p 131 - 132)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Unsqueeze adds batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # Removes batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=  GPT_CONFIG_124M['context_length']\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 133 examples\n",
    "\n",
    "inputs = torch.tensor([[16833,3626, 6100],\n",
    "                       [40,1107,588]])\n",
    "\n",
    "targets = torch.tensor([[3626,6100,345],\n",
    "                        [1107,588,11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Create logits, convert into prob scores\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [45647],\n",
      "         [ 1804]],\n",
      "\n",
      "        [[49906],\n",
      "         [15335],\n",
      "         [20861]]])\n"
     ]
    }
   ],
   "source": [
    "# Get token IDs using argmax\n",
    "token_ids = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token IDs:\\n\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1: effort moves you\n",
      "Outputs batch 1: Armed sd doing\n"
     ]
    }
   ],
   "source": [
    "# Convert token ids back to text\n",
    "print(f\"Targets batch 1:{token_ids_to_text(targets[0],tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Get initial softmax prob scores corresp to target tokens\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1:\",target_probas_1)\n",
    "\n",
    "text_idx=1\n",
    "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2:\",target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.2727, -10.4829, -10.8902, -11.3943, -10.0409, -12.0300])\n"
     ]
    }
   ],
   "source": [
    "# Need to convert target probas to log\n",
    "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.6852)\n"
     ]
    }
   ],
   "source": [
    "# Get avg of log probas\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print((avg_log_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6852)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Avg neg log loss is similar to cross-entropy and used interchangeably\n",
    "\n",
    "# CHeck shape of logitsand targets\n",
    "\n",
    "print(\"Logits shape: \",logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets:, torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# For cross entropy need to flatten tensors by combining over batch dimension\n",
    "\n",
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten(0,1)\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:,\", targets_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6852)\n"
     ]
    }
   ],
   "source": [
    "# use cross entropy function\n",
    "# In one step this applies softmax, selects prob score corresp to target id, and compute neg avg log prob\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43702.9180)\n"
     ]
    }
   ],
   "source": [
    "# Perplexity (p 139) -  how well does model's prob dist match actual prob dist\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# Reuse text from The Verdict (p 141)\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n"
     ]
    }
   ],
   "source": [
    "print(text_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_ratio = 0.90\n",
    "split_index = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_index]\n",
    "val_data = text_data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders (p 143)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Confirm dataloaders created correctly\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x,y in val_loader:\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to calculate cross entropy loss for a given batch (p 144)\n",
    "\n",
    "def calc_loss_batch(input_batch,target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1),target_batch.flatten()  \n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to compute loss for all batches in dataloader rather than just one\n",
    "# p 144\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "# Apply this function to training and validation loaders (p 145)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader,model,device)\n",
    "    val_loss = calc_loss_loader(val_loader,model,device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to show training and validation set losses (p 148)\n",
    "\n",
    "#def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "#    model.eval()\n",
    "#    with torch.no_grad():\n",
    "#       train_loss = calc_loss_loader(\n",
    "#            train_loader, model, device, num_batches=eval_iter\n",
    "#        )\n",
    "#        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter\n",
    "#                                    )\n",
    "#        model.train()\n",
    "#        return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to track model improvement\n",
    "\n",
    "#def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "#    model.eval()\n",
    "#    context_size = model.pos_emb.weight.shape[0]\n",
    "#    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "#    with torch.no_grad():\n",
    "#        token_ids = generate_text_simple(\n",
    "#            model=model, idx=encoded,\n",
    "#            max_new_tokens=50, context_size=context_size\n",
    "#        )\n",
    "#    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "#   print(decoded_text.replace(\"\\n\", \" \"))\n",
    "#    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an LLM (5.2) - p 146\n",
    "\n",
    "def train_model_simple(model,train_loader,val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [],[],[]\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):  \"\n",
    "                    f\"Train loss {train_loss:.3f},\"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                    )\n",
    "        \n",
    "        generate_and_print_sample(model,tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter\n",
    "                                    )\n",
    "        model.train()\n",
    "        return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):  Train loss 9.783,Val loss 9.927\n",
      "Ep 1 (Step 000005):  Train loss 7.985,Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):  Train loss 6.753,Val loss 7.048\n",
      "Ep 2 (Step 000015):  Train loss 6.114,Val loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020):  Train loss 5.525,Val loss 6.490\n",
      "Ep 3 (Step 000025):  Train loss 5.324,Val loss 6.387\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030):  Train loss 4.761,Val loss 6.360\n",
      "Ep 4 (Step 000035):  Train loss 4.461,Val loss 6.258\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step 000040):  Train loss 3.833,Val loss 6.196\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045):  Train loss 3.352,Val loss 6.139\n",
      "Ep 6 (Step 000050):  Train loss 2.861,Val loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055):  Train loss 2.347,Val loss 6.138\n",
      "Ep 7 (Step 000060):  Train loss 2.084,Val loss 6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065):  Train loss 1.521,Val loss 6.176\n",
      "Ep 8 (Step 000070):  Train loss 1.272,Val loss 6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075):  Train loss 1.000,Val loss 6.277\n",
      "Ep 9 (Step 000080):  Train loss 0.718,Val loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085):  Train loss 0.506,Val loss 6.325\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
     ]
    }
   ],
   "source": [
    "# Test  - run GPTModel instance for 10 epochs (p 149)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "#print(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create charts to show training and validation losses (p 150)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen,train_losses,label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen,val_losses,linestyle=\"-.\",label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen,train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample graph\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor,tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding strategies to control randomness (p 151)\n",
    "\n",
    "#model.to(\"cpu\")\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Put GPT model into generate_text_simple model\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    "\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature scaling\n",
    "# previously used argmax to select highest prob token (greedy decoding), now use probabilistic sampling\n",
    "\n",
    "vocab = {\n",
    "    \"closer\":0,\n",
    "    \"every\":1,\n",
    "    \"effort\":2,\n",
    "    \"forward\":3,\n",
    "    \"inches\":4,\n",
    "    \"moves\":5,\n",
    "    \"pizza\":6,\n",
    "    \"toward\":7,\n",
    "    \"you\":8,\n",
    "}\n",
    "inverse_vocab = {v:k for k,v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'closer', 1: 'every', 2: 'effort', 3: 'forward', 4: 'inches', 5: 'moves', 6: 'pizza', 7: 'toward', 8: 'you'}\n"
     ]
    }
   ],
   "source": [
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 153 - sample logits based on \"Every effort moves you\"\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51,0.89,-1.90,6.75,1.63,-1.62,-1.89,6.28,1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas:\n",
      " tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n",
      "            0.0001,     0.3576,     0.0040])\n",
      "Next token id:\n",
      " 3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Convert to probs\n",
    "probas = torch.softmax(next_token_logits,dim=0)\n",
    "print(\"Probas:\\n\",probas)\n",
    "#  Get ID for next token\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(\"Next token id:\\n\",next_token_id)\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "toward\n"
     ]
    }
   ],
   "source": [
    "# Now make this probabilistic rather than just using argmax\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas,num_samples=1).item()\n",
    "print(next_token_id)\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 samples to see how many times it chooses each possible word\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas,num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temperature to dist = div logits by some num > 0. Temp >1 = more uniform distrib\n",
    "\n",
    "def softmax_with_temperature(logits,temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1,0.1,5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits,T)\n",
    "                 for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax  = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x+i * bar_width, scaled_probas[i],\n",
    "                   bar_width,label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(),rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# Top k-sampling - restrict sampling to k most likely tokens and mask the rest with -inf\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions\", top_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# Set values below threshold to -inf\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other = next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# p 157 - now convert top-k to new probs\n",
    "topk_probas = torch.softmax(new_logits,dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine temp sampling and topk\n",
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0,top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx,idx_next),dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you----- I-- of he his to his a heis\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Test new function\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 5.4 - Loading & Saving Model Weights (p 159)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "},\n",
    "\"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\",map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pretrained Weights from OpenAI (p161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")#\n",
    "#\n",
    "#print(f\"Using {device} device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x189cac9d0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#start_time = time.time()\n",
    "#from gpt_download import download_and_load_gpt2\n",
    "#end_time = time.time()\n",
    "\n",
    "#print(f\"Import took {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT wts from get_download.py file\n",
    "\n",
    "#settings, params = download_and_load_gpt2(\n",
    "#    model_size=\"124M\", models_dir='gpt2'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspects settings and params\n",
    "\n",
    "#print(\"Settings:\", settings)\n",
    "#print(\"Parameter Dictionary Keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to transfer setting and params info into our GPT model\n",
    "# Create a dict\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading smallest model\n",
    "#model_name = \"gpt2-small (124M)\"\n",
    "#NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "#NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config length\n",
    "#NEW_CONFIG.update({\"context_length\":1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bias param\n",
    "#NEW_CONFIG.udpate({\"qkv_bias\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use updated config to create new model (p 164)\n",
    "#gpt = GPTModel(NEW_CONFIG)\n",
    "#gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override random weights with weights loaded into params dict (p165)\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: Left {left.shape}, \"\n",
    "                        \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to load weights from params dict into GPT model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight,params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight,params['wte'])\n",
    "\n",
    "    for v in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b]['attn']['c_attn'])['w'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b]['attn']['c_attn'])['b'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b]['attn']['c_proj']['w'].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b]['attn']['c_proj']['b'])\n",
    "        \n",
    "        gpt.trf_blocks[b].ffn.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[0].weight,\n",
    "            params[\"blocks\"][b]['mlp']['c_fc']['w'].T)\n",
    "        gpt.trf_blocks[b].ffn.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[0].bias,\n",
    "            params[\"blocks\"][b]['mlp']['c_fc']['b'])\n",
    "        gpt.trf_blocks[b].ffn.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[2].weight,\n",
    "            params[\"blocks\"][b]['mlp']['c_proj']['w'].T)\n",
    "        gpt.trf_blocks[b].ffn.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ffn.layers[2].bias,\n",
    "            params[\"blocks\"][b]['mlp']['c_proj']['b'])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b]['ln_1']['g'])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b]['ln_1']['b'])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b]['ln_2']['g'])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b]['ln_2']['b']\n",
    "        )\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params['g'])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params['b'])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params['wte'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test function above and load OpenAI model wts into GPT model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m load_weights_into_gpt(\u001b[43mgpt\u001b[49m,params)\n\u001b[1;32m      3\u001b[0m gpt\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "# Test function above and load OpenAI model wts into GPT model\n",
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate text using generate function\u001b[39;00m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      4\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m generate(\n\u001b[0;32m----> 5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mgpt\u001b[49m,\n\u001b[1;32m      6\u001b[0m     idx\u001b[38;5;241m=\u001b[39mtext_to_token_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery effort moves you\u001b[39m\u001b[38;5;124m\"\u001b[39m,tokenizer)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m      7\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m      8\u001b[0m     context_size\u001b[38;5;241m=\u001b[39mGPT_CONFIG_124M[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     10\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate text using generate function\n",
    "\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length'],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enviro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
