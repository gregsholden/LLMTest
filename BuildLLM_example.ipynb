{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 20479\n",
      "I HAD alw\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and review the first 100 characters\n",
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Length of text:\", len(raw_text))\n",
    "print(raw_text[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3]\n",
    "tens1 = torch.tensor(data)\n",
    "print(tens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "array = np.array([1,2,3])\n",
    "print(array)\n",
    "tens2 = torch.from_numpy(array)\n",
    "print(tens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ch 2, p 22 - split text\n",
    "import re\n",
    "text = \"Hello, world.  This, is a test.\"\n",
    "result = re.split(r'([!,.]|\\s)', text)\n",
    "print(result)\n",
    "result2 = [item for item in result if item.strip()]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Add more puncuation and  --- to the split (p 23)\n",
    "text = \"Hello, world.  Is this-- a test?\"\n",
    "result = re.split(r'([!,.?:]|--|\\s)', text)\n",
    "print(result)\n",
    "result2 = [item for item in result if item.strip()]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "# p24 - Use tokenizer scheme on Wharton text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Test output of tokenized used on The Verdict  \n",
    "print(preprocessed[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "['yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# 2.3 - Create a vocabulary (p 25)\n",
    "\n",
    "# Create list of all uniuqe words and sort\n",
    "all_words   = sorted(set(preprocessed))\n",
    "vocab_size  = len(all_words)\n",
    "print(vocab_size)\n",
    "print(all_words[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Use vocab to create a dictionary\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 27 - Make simple tokenizer Class\n",
    "class SimpleTokenV1:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores the vocab dictionary\n",
    "        self.int_to_str = {integer: string for string, integer in vocab.items()} # reverse dictionary (token IDS to text tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids # return the token IDs\n",
    "\n",
    "    def decoder(self, ids):  # Converts token IDs back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "tokenizer = SimpleTokenV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# Need to account for words not in the vocab (p 30)\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that new characters are in the vocab\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 31 - Adjust tokenizer class to reflect UNK token\n",
    "class SimpleTokenizerV2:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores the vocab dictionary\n",
    "        self.int_to_str = {integer: string for string, integer in vocab.items()} # reverse dictionary (token IDS to text tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Account for words not in the vocab\n",
    "        preprocessed = [token if token in self.str_to_int else \"<|unk|>\" for token in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids # return the token IDs\n",
    "\n",
    "    def decoder(self, ids):  # Converts token IDs back to text\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer adjusted for unknown words\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "# Get token IDs\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "print(tokenizer.decoder(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 - p 33 - Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 1279, 91, 437, 1659, 5239, 60, 29, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# Encode text to integers (p 33)\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext]> In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext]> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# Decode integers back to text\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Use BPE on The Verdict (Ch 2.6 - p 35)\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "print(type(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# Take sample of encoded text\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -------> 4920\n",
      "[290, 4920] -------> 2241\n",
      "[290, 4920, 2241] -------> 287\n",
      "[290, 4920, 2241, 287] -------> 257\n"
     ]
    }
   ],
   "source": [
    "#  Test print integers\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"------->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ------->  established\n",
      " and established ------->  himself\n",
      " and established himself ------->  in\n",
      " and established himself in ------->  a\n"
     ]
    }
   ],
   "source": [
    "# Convert back to text using decoder\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"------->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset Class (Ch. 2.6, p 37-38)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"}) # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt) # Tokenize the entire text\n",
    "        \n",
    "        # Uses overlapping chunks of max_length tokens\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) \n",
    "\n",
    "    # Returns total number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # Returns a single row of the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader (Ch 2.6 - p38)\n",
    "\n",
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiate BPE tokenizer\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # Creates dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                              shuffle=shuffle, drop_last=drop_last, # drop_last=True to drop the last incomplete batch\n",
    "                                                num_workers=num_workers) # num_workers=0 to use single process\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader with batch size of 1 (p 39)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "# Fetch another batch\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# Test with larger batch size and stride, so no overlap\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"\\nTargets:\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2.7 (p41) - Create token embeddings from token IDs\n",
    "\n",
    "# Sample input layer\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 (p 46) - Need to add positional encoding to the token id embeddings\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Example (p 46)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape) \n",
    "print(type(inputs)) \n",
    "print(type(targets)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Embed token IDs into 256-dimensional vectors (p 46 - 47)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Absolute embedding position\n",
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "#print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 3 Coding Attention Mechanism (p 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# p57 - Simple self-attention mechanism without trainable weights\n",
    "import torch\n",
    "\n",
    "# Create a tensor with 3 token embeddings\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], #  journey (x^2)\n",
    "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
    "     [0.22, 0.58, 0.33],  # with (x^4)\n",
    "     [0.77, 0.25, 0.10],  # one (x^5)\n",
    "     [0.05, 0.80, 0.55]],  # step (x^6) \n",
    "     \n",
    ")\n",
    "\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "torch.Size([3])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([6])\n",
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# p 58\n",
    "\n",
    "query = inputs[1]\n",
    "print(query)\n",
    "print(query.shape)\n",
    "\n",
    "att_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(att_scores_2)\n",
    "print(att_scores_2.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(i,x_i)\n",
    "    att_scores_2[i] = torch.dot(x_i, query)\n",
    "print(att_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "attn_weights_2_tmp = att_scores_2 / att_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum\",attn_weights_2_tmp.sum())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Use softmax function to normalize the attention scores (p60)\n",
    "def softmax_naive(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / exp_x.sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(att_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Better to use PyTorch's softmax function\n",
    "attn_weights_2 = torch.softmax(att_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "Context_vec_2 shape torch.Size([3])\n",
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor(0.1385)\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor(0.2379)\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor(0.2333)\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor(0.1240)\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor(0.1082)\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor(0.1581)\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Compute context vectors\n",
    "query = inputs[1]\n",
    "print(query)\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "print(\"Context_vec_2 shape\",context_vec_2.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(i, x_i)\n",
    "    print(attn_weights_2[i])\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context vectors for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Faster way using matrix multiplication\n",
    "attn_scores = torch.matmul(inputs, inputs.T)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Compute context vectors\n",
    "all_context_vecs = torch.matmul(attn_weights, inputs)\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt 2 (3.4 p 64) - Implementing the self atttention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "d_in 3\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "print(x_2)\n",
    "d_in = x_2.shape[0]\n",
    "print(\"d_in\",d_in)\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query: Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "W_key Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "W_value Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_query:\", W_query)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_key\",W_key)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out),requires_grad=False)\n",
    "print(\"W_value\",W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tensor([0.4306, 1.4551])\n",
      "Query_v2: tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Compute Query, Key, and Value vectors\n",
    "query_2 = torch.matmul(x_2, W_query)\n",
    "query_2_v2 = x_2 @ W_query\n",
    "key_2 = torch.matmul(x_2, W_key)\n",
    "value_2 = torch.matmul(x_2, W_value)\n",
    "\n",
    "print(\"Query:\", query_2)\n",
    "print(\"Query_v2:\", query_2_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "Keys shape: torch.Size([6, 2])\n",
      "Values: tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n",
      "Values shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Compute all keys and values\n",
    "keys = torch.matmul(inputs, W_key)\n",
    "print(\"Keys:\", keys)\n",
    "print(\"Keys shape:\", keys.shape)\n",
    "values = torch.matmul(inputs, W_value)\n",
    "print(\"Values:\", values)\n",
    "print(\"Values shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys_2: tensor([0.4433, 1.1419])\n",
      "Attention scores: tensor(1.8524)\n",
      "Attention scores_v2: tensor(1.8524)\n",
      "Attention scores_v3: tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Calc attention scores - unnormalized\n",
    "keys_2 = keys[1]\n",
    "print(\"keys_2:\", keys_2)\n",
    "attn_scores_2 = query_2.dot(keys_2)\n",
    "attn_scores_2_v2 = query_2 @ keys_2\n",
    "attn_scores_2_v3 = torch.matmul(query_2, keys_2)\n",
    "print(\"Attention scores:\", attn_scores_2)\n",
    "print(\"Attention scores_v2:\", attn_scores_2_v2)\n",
    "print(\"Attention scores_v3:\", attn_scores_2_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# All attention scores\n",
    "att_scores_2 = query_2 @ keys.T\n",
    "print(\"Attention scores:\", att_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Attention weights: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "Attention weights shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores\n",
    "d_k = keys.shape[1]\n",
    "print(d_k)\n",
    "attn_weights_2 = torch.softmax(att_scores_2 / (d_k ** 0.5), dim=-1)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Attention weights shape:\", attn_weights_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Single context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Class for self-attention mechanism (p 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the newly created class\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make better version using more stable nn.Linear - using diff wt initialization scheme (p 72 - 73)\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / (d_k ** 0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Hiding future words with Causal Masking (p 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute attention weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)  \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create mask using tril so values above diagonal are zero\n",
    "context_length = attn_scores.shape[0]\n",
    "print(context_length)\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) \n",
    "print(mask_simple)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multiply mask with attention weights\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalize the masked attention weights\n",
    "rows_sums = masked_simple.sum(dim=-1,keepdim=True)\n",
    "masked_simple_norm = masked_simple / rows_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# More efficient way to do it - use negative infinity before softmax\n",
    "mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
    "masked =attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to this more efficient mask\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking with dropout (p 78)\n",
    "# used (1) after calculating attention weights and (2) after applying attention weights to value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a mask with 50% of values set to zero - simple example\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply dropout to attention weights - actual\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#  Create compact casual attention class\n",
    "# Need to allow for batch inputs\n",
    "\n",
    "batch = torch.stack((inputs,inputs),dim=0)\n",
    "print(batch[0])\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New class with Causal Masking and Dropout (p 81)\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        print(\"d_out\",d_out)\n",
    "        print(\"context_length\",context_length)\n",
    "        print(\"dropout\",dropout)\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "        # Self register buffer used to keep buffer tensors and model params on same device (GPU vs CPU)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "        #print(\"mask\",self.mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(\"batch:\",b)\n",
    "        print(\"num_tokens:\",num_tokens)\n",
    "        print(\"d_in:\",d_in)\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        #print(\"keys shape\",keys.shape)\n",
    "        #print(\"keys:\",keys)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.transpose(1,2)\n",
    "\n",
    "        #mask = torch.triu(torch.ones(attn_scores.shape[0], attn_scores.shape[1]),diagonal=1)\n",
    "        attn_scores.masked_fill_ = (self.mask.bool()[:num_tokens,:num_tokens], -torch.inf)\n",
    "        #print(\"attn_scores shape\",attn_scores.shape)\n",
    "        \n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(masked / (d_k ** 0.5), dim=-1)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "first context_vecs: tensor([[-0.4519,  0.2216],\n",
      "        [-0.5695,  0.0343],\n",
      "        [-0.6141, -0.0377],\n",
      "        [-0.5642, -0.0717],\n",
      "        [-0.5490, -0.0906],\n",
      "        [-0.5291, -0.0961]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "print(context_length)\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"first context_vecs:\", context_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention class (p 84) -- output context vectors are concatenated\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vecs = [head(x) for head in self.heads]\n",
    "        context_vecs = torch.cat(context_vecs, dim=-1)\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "d_out 2\n",
      "context_length 6\n",
      "dropout 0.0\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "batch: 2\n",
      "num_tokens: 6\n",
      "d_in: 3\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5695,  0.0343,  0.5668,  0.2819],\n",
      "         [-0.6141, -0.0377,  0.6008,  0.3481],\n",
      "         [-0.5642, -0.0717,  0.5462,  0.3445],\n",
      "         [-0.5490, -0.0906,  0.5318,  0.3359],\n",
      "         [-0.5291, -0.0961,  0.5093,  0.3362]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5695,  0.0343,  0.5668,  0.2819],\n",
      "         [-0.6141, -0.0377,  0.6008,  0.3481],\n",
      "         [-0.5642, -0.0717,  0.5462,  0.3445],\n",
      "         [-0.5490, -0.0906,  0.5318,  0.3359],\n",
      "         [-0.5291, -0.0961,  0.5093,  0.3362]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # Number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\",context_vecs.shape)\n",
    "# First dimension of 2 for batches, second of 6 is num of elements, 4 is 2 output contcext vectors for each of the 2 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class (p 86)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Confirm that d_out is divisible by num_heads\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)  \n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vecs = (attn_weights @ values).transpose(1,2)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        \n",
    "        return context_vecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "context_length 6\n",
      "d_in 3\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test (p 90)  \n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"batch_size\",batch_size)\n",
    "print(\"context_length\",context_length)  \n",
    "print(\"d_in\",d_in)\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x_trans = x.transpose(0,1)\n",
    "print(x_trans)\n",
    "print(x_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x_view = x.view(3,2)\n",
    "print(x_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# Multiply\n",
    "x_mm = x @ x.T\n",
    "print(x_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 4 - Coding an LLM Architecture (p 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 1024, # Max num of input tokens\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 96\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"],bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# p 97\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "# Create small sample text to run through the model (p 97)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "Logits: tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and feed it text (p 98)\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch example tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
      "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264]])\n",
      "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test exampple of a NN layer\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5)\n",
    "print(\"batch example\",batch_example)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.2432],\n",
      "        [0.1928]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0799],\n",
      "        [0.0670]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calc mean and variance\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs tensor([[-0.8603, -0.8603,  0.5869,  1.4698,  0.5242, -0.8603],\n",
      "        [-0.7450, -0.7450, -0.0102,  0.4844,  1.7608, -0.7450]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[ 0.0000e+00],\n",
      "        [-9.9341e-09]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs\",out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn off scientific notation for better readability\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer norm class (p 103)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True,unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.9998],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use newly created layer norm class\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False ,keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feed forward network with GELU activation (p 105)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot GELU vs ReLU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]),1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f{label}(x)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GELU in feed forward network (p 107)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"],cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test FFN (p 108)\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut connection (p 109) aka skip or residual connection (used to mitigate vanishing gradient problem)\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "    \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),\n",
    "                           nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),\n",
    "                            nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),\n",
    "                            nn.GELU())\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that computes gradients in backwards pass (p 111)\n",
    "def print_gradients(model, x):\n",
    "\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020174124801997095\n",
      "layers.1.0.weight has gradient mean of 0.00012011772923870012\n",
      "layers.2.0.weight has gradient mean of 0.0007152438047342002\n",
      "layers.3.0.weight has gradient mean of 0.0013988513965159655\n",
      "layers.4.0.weight has gradient mean of 0.005049603525549173\n"
     ]
    }
   ],
   "source": [
    "# Use the function to compute gradients\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22186797857284546\n",
      "layers.1.0.weight has gradient mean of 0.20709273219108582\n",
      "layers.2.0.weight has gradient mean of 0.3292388319969177\n",
      "layers.3.0.weight has gradient mean of 0.2667771577835083\n",
      "layers.4.0.weight has gradient mean of 1.3268064260482788\n"
     ]
    }
   ],
   "source": [
    "# Compare by running with skip connections\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer block class (p 115)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"], \n",
    "            d_out = cfg[\"emb_dim\"], \n",
    "            context_length= cfg[\"context_length\"], \n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"], \n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        #x = self.att(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example (p 116)\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 256, # Max num of input tokens\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined GPT model implementation (p 119)\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape:\n",
      " torch.Size([2, 4, 50257])\n",
      "tensor([[[    -0.1795,      0.2852,     -0.7613,  ...,     -0.4837,\n",
      "              -0.4250,     -0.1719],\n",
      "         [    -0.6258,     -0.3748,     -0.9702,  ...,      0.1917,\n",
      "              -1.3234,     -0.2764],\n",
      "         [     0.5174,      0.1387,      0.2489,  ...,      0.3505,\n",
      "              -0.0775,     -0.0800],\n",
      "         [    -0.2566,     -0.6969,     -0.9948,  ...,     -0.0447,\n",
      "               0.0618,      0.1347]],\n",
      "\n",
      "        [[    -0.2238,      0.1165,     -0.9984,  ...,     -0.1573,\n",
      "              -0.4480,     -0.0286],\n",
      "         [    -0.8723,     -0.3939,     -1.1099,  ...,      0.3303,\n",
      "              -0.0924,     -0.0000],\n",
      "         [     0.4599,     -0.1427,     -0.1223,  ...,      0.2749,\n",
      "               0.0583,     -0.0899],\n",
      "         [    -0.6216,     -0.4485,     -0.4767,  ...,     -0.3652,\n",
      "               0.3440,     -0.3815]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "print(batch)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"Output shape:\\n\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "# Count params\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# But the 163 mm figure above double counts the parameters in the output layer. \n",
    "# This is called weight tying and is a common technique in LLMs to reduce the number of parameters.\n",
    "# SIG - reuses weights from embedding layer in output layer\n",
    "\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable paramsconsidering weight tying: 123,822,336\n"
     ]
    }
   ],
   "source": [
    "# Remove output layer param count\n",
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "                       for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(f\"Number of trainable params\"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 619.58 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate total size of model\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate text (p 124)\n",
    "\n",
    "def generate_text_simple(model,idx,max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] # Crops context if too large\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1, :]  # Focus on last time step\n",
    "        probas = torch.softmax(logits, dim=-1)  # Shape is (batch, vocab_size)\n",
    "        idx_next = torch.argmax(probas, dim=-1,keepdim=True)  # Shape (batch, 1)\n",
    "        idx = torch.cat((idx, idx_next),dim=1)  # Appends newest work, idx shape (batch, n_tokens + 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape:  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example (p 125)\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \",encoded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# eval mode disables random components\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size = GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Laur inhab DistrinetalkQueue\n"
     ]
    }
   ],
   "source": [
    "# Convert IDs back to text using decoder\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces gibberish because using random initial weights and no training (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 5 Pretraining on unlabeled data (p128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Same as BPE vocab size\n",
    "    \"context_length\": 256, # Changed from 1024 in earlier model\n",
    "    \"emb_dim\": 768, # Embedding size for each token\n",
    "    \"n_heads\": 12,  # Num of attention heads in each multi-head attention layer\n",
    "    \"n_layers\": 12, # Num of transformer blocks\n",
    "    \"drop_rate\": 0.1,  # 0.1 means 10% dropped\n",
    "    \"qkv_bias\": False    # Whether to add a learnable bias to the query, key, and value projections\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # Disables dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "# Create utility functions fpr text to token ID conversions (p 131 - 132)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Unsqueeze adds batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # Removes batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=  GPT_CONFIG_124M['context_length']\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 133 examples\n",
    "\n",
    "inputs = torch.tensor([[16833,3626, 6100],\n",
    "                       [40,1107,588]])\n",
    "\n",
    "targets = torch.tensor([[3626,6100,345],\n",
    "                        [1107,588,11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Create logits, convert into prob scores\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "# Get token IDs using argmax\n",
    "token_ids = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token IDs:\\n\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1: effort moves you\n",
      "Outputs batch 1: Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Convert token ids back to text\n",
    "print(f\"Targets batch 1:{token_ids_to_text(targets[0],tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Get initial softmax prob scores corresp to target tokens\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1:\",target_probas_1)\n",
    "\n",
    "text_idx=1\n",
    "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2:\",target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Need to convert target probas to log\n",
    "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Get avg of log probas\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print((avg_log_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Avg neg log loss is similar to cross-entropy and used interchangeably\n",
    "\n",
    "# CHeck shape of logitsand targets\n",
    "\n",
    "print(\"Logits shape: \",logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets:, torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# For cross entropy need to flatten tensors by combining over batch dimension\n",
    "\n",
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten(0,1)\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:,\", targets_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# use cross entropy function\n",
    "# In one step this applies softmax, selects prob score corresp to target id, and compute neg avg log prob\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "# Perplexity (p 139) -  how well does model's prob dist match actual prob dist\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# Reuse text from The Verdict (p 141)\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n"
     ]
    }
   ],
   "source": [
    "print(text_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_ratio = 0.90\n",
    "split_index = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_index]\n",
    "val_data = text_data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders (p 143)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Confirm dataloaders created correctly\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x,y in val_loader:\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to calculate cross entropy loss for a given batch (p 144)\n",
    "\n",
    "def calc_loss_batch(input_batch,target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1),target_batch.flatten()  \n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to compute loss for all batches in dataloader rather than just one\n",
    "# p 144\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583372328016\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "# Apply this function to training and validation loaders (p 145)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader,model,device)\n",
    "    val_loss = calc_loss_loader(val_loader,model,device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to show training and validation set losses (p 148)\n",
    "\n",
    "#def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "#    model.eval()\n",
    "#    with torch.no_grad():\n",
    "#       train_loss = calc_loss_loader(\n",
    "#            train_loader, model, device, num_batches=eval_iter\n",
    "#        )\n",
    "#        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter\n",
    "#                                    )\n",
    "#        model.train()\n",
    "#        return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to track model improvement\n",
    "\n",
    "#def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "#    model.eval()\n",
    "#    context_size = model.pos_emb.weight.shape[0]\n",
    "#    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "#    with torch.no_grad():\n",
    "#        token_ids = generate_text_simple(\n",
    "#            model=model, idx=encoded,\n",
    "#            max_new_tokens=50, context_size=context_size\n",
    "#        )\n",
    "#    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "#   print(decoded_text.replace(\"\\n\", \" \"))\n",
    "#    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an LLM (5.2) - p 146\n",
    "\n",
    "def train_model_simple(model,train_loader,val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [],[],[]\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):  \"\n",
    "                    f\"Train loss {train_loss:.3f},\"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                    )\n",
    "        \n",
    "        generate_and_print_sample(model,tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter\n",
    "                                    )\n",
    "        model.train()\n",
    "        return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):  Train loss 9.781,Val loss 9.933\n",
      "Ep 1 (Step 000005):  Train loss 8.111,Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):  Train loss 6.661,Val loss 7.048\n",
      "Ep 2 (Step 000015):  Train loss 5.961,Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n"
     ]
    }
   ],
   "source": [
    "# Test  - run GPTModel instance for 10 epochs (p 149)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "#print(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create charts to show training and validation losses (p 150)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen,train_losses,label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen,val_losses,linestyle=\"-.\",label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen,train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sample graph\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor,tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding strategies to control randomness (p 151)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and,\n"
     ]
    }
   ],
   "source": [
    "# Put GPT model into generate_text_simple model\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    "\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature scaling\n",
    "# previously used argmax to select highest prob token (greedy decoding), now use probabilistic sampling\n",
    "\n",
    "vocab = {\n",
    "    \"closer\":0,\n",
    "    \"every\":1,\n",
    "    \"effort\":2,\n",
    "    \"forward\":3,\n",
    "    \"inches\":4,\n",
    "    \"moves\":5,\n",
    "    \"pizza\":6,\n",
    "    \"toward\":7,\n",
    "    \"you\":8,\n",
    "}\n",
    "inverse_vocab = {v:k for k,v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'closer', 1: 'every', 2: 'effort', 3: 'forward', 4: 'inches', 5: 'moves', 6: 'pizza', 7: 'toward', 8: 'you'}\n"
     ]
    }
   ],
   "source": [
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p 153 - sample logits based on \"Every effort moves you\"\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51,0.89,-1.90,6.75,1.63,-1.62,-1.89,6.28,1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas:\n",
      " tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n",
      "            0.0001,     0.3576,     0.0040])\n",
      "Next token id:\n",
      " 3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Convert to probs\n",
    "probas = torch.softmax(next_token_logits,dim=0)\n",
    "print(\"Probas:\\n\",probas)\n",
    "#  Get ID for next token\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(\"Next token id:\\n\",next_token_id)\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Now make this probabilistic rather than just using argmax\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas,num_samples=1).item()\n",
    "print(next_token_id)\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 samples to see how many times it chooses each possible word\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas,num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temperature to dist = div logits by some num > 0. Temp >1 = more uniform distrib\n",
    "\n",
    "def softmax_with_temperature(logits,temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1,0.1,5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits,T)\n",
    "                 for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax  = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x+i * bar_width, scaled_probas[i],\n",
    "                   bar_width,label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(),rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# Top k-sampling - restrict sampling to k most likely tokens and mask the rest with -inf\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions\", top_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# Set values below threshold to -inf\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other = next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# p 157 - now convert top-k to new probs\n",
    "topk_probas = torch.softmax(new_logits,dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine temp sampling and topk\n",
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0,top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx,idx_next),dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you, I. I it a,.\" me \" my a \" had\n"
     ]
    }
   ],
   "source": [
    "# Test new function\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 5.4 - Loading & Saving Model Weights (p 159)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "},\n",
    "\"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\",map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pretrained Weights from OpenAI (p161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")#\n",
    "\n",
    "print(f\"Using {device} device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x3473f9cd0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from gpt_download import download_and_load_gpt2\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Import took {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|| 77.0/77.0 [00:00<00:00, 20.1kiB/s]\n",
      "encoder.json: 100%|| 1.04M/1.04M [00:00<00:00, 3.41MiB/s]\n",
      "hparams.json: 100%|| 90.0/90.0 [00:00<00:00, 72.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|| 498M/498M [01:26<00:00, 5.78MiB/s] \n",
      "model.ckpt.index: 100%|| 5.21k/5.21k [00:00<00:00, 1.94MiB/s]\n",
      "model.ckpt.meta: 100%|| 471k/471k [00:00<00:00, 2.09MiB/s]\n",
      "vocab.bpe: 100%|| 456k/456k [00:00<00:00, 2.05MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load GPT wts from get_download.py file\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir='gpt2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter Dictionary Keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "# Inspects settings and params\n",
    "\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter Dictionary Keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to transfer setting and params info into our GPT model\n",
    "# Create a dict\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading smallest model\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config length\n",
    "NEW_CONFIG.update({\"context_length\":1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bias param\n",
    "NEW_CONFIG.update({\"qkv_bias\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use updated config to create new model (p 164)\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override random weights with weights loaded into params dict (p165)\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: Left {left.shape}, \"\n",
    "                        \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to load weights from params dict into GPT model (Ch 5.5, p 165)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight,params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight,params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b]['attn']['c_attn'])['w'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b]['attn']['c_attn'])['b'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b]['attn']['c_proj']['w'].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b]['attn']['c_proj']['b'])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b]['mlp']['c_fc']['w'].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b]['mlp']['c_fc']['b'])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b]['mlp']['c_proj']['w'].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b]['mlp']['c_proj']['b'])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b]['ln_1']['g'])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b]['ln_1']['b'])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b]['ln_2']['g'])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b]['ln_2']['b']\n",
    "        )\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params['g'])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params['b'])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params['wte'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function above and load OpenAI model wts into GPT model\n",
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text using generate function\n",
    "\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length'],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 6 Fine Tuning For Classification (p 169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into pandas df (p 173)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to randomly split data into train and test sets (p 175)\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# When batching need to either truncate longer messages or pad shorter ones. \n",
    "# Padding is more common.  Padding is done by adding special token to end of message\n",
    "\n",
    "# Check token number for endoftext\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset class (p 177)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "        # Note: A more pythonic version to implement this method\n",
    "        # is the following, which is also used in the next chapter:\n",
    "        # return max(len(encoded_text) for encoded_text in self.encoded_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "# Note:  Text sequences all need to ne the same length\n",
    "\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create val and test datasets (p 178) - max_length same as training dataset\n",
    "\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader for training dataset (p 180)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Confirm dataloaders created correctly (p 180)\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "# Check how many batches in each set\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 p 181 - Initialize model with pretrained weights\n",
    "\n",
    "# Set config\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.data-00000-of-00001: 100%|| 498M/498M [01:06<00:00, 7.48MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Reuse GPT class then download weights (p 182)\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "#from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "#  Test that imported model generates coherent text (p 182)\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "# Try to prompt model and see if it can classify as spam or not\n",
    "\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Review model architecture (p 185)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze wts to prep for classification tuning (p 186)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outputlayer (originally 768 mapping to vocab size of 50257) with new layer (0,1)\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG['emb_dim'],\n",
    "    out_features=num_classes,\n",
    ")\n",
    "# requires_grad = True by default, which makes only this last layer trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also need to make final norm layer and transformer blocks trainable\n",
    "\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Can still use the model as before\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# Show that there are only 2 output dimensions now instead of the 50257 previously\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# Get info about last token (p 188)\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to probabilities (p 192) using softmax\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Same things but without softmax\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy (p 192)\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps device.\n",
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# Calc accuracy (p 193)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# As of this writing, in PyTorch 2.4, the results obtained via CPU and MPS were identical.\n",
    "# However, in earlier versions of PyTorch, you may observe different results when using MPS.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Running on {device} device.\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fine tune.  Cant differentiate so min cross entropy loss (p 194)\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc loss for all batches (p194)\n",
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "# Calc loss for training and validation and test sets (p 194)\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 - Fine Tuning (p 195) on supervised data to lower cross entropy loss and increase accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune to classify spam or not spam (p 196)\n",
    "\n",
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval model (p 197)\n",
    "\n",
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.402, Val loss 0.420\n",
      "Ep 1 (Step 000050): Train loss 0.374, Val loss 0.418\n",
      "Ep 1 (Step 000100): Train loss 0.363, Val loss 0.433\n",
      "Training accuracy: 87.50% | Validation accuracy: 90.00%\n",
      "Ep 2 (Step 000150): Train loss 0.466, Val loss 0.404\n",
      "Ep 2 (Step 000200): Train loss 0.424, Val loss 0.403\n",
      "Ep 2 (Step 000250): Train loss 0.436, Val loss 0.395\n",
      "Training accuracy: 82.50% | Validation accuracy: 82.50%\n",
      "Ep 3 (Step 000300): Train loss 0.391, Val loss 0.389\n",
      "Ep 3 (Step 000350): Train loss 0.383, Val loss 0.388\n",
      "Training accuracy: 85.00% | Validation accuracy: 82.50%\n",
      "Ep 4 (Step 000400): Train loss 0.280, Val loss 0.405\n",
      "Ep 4 (Step 000450): Train loss 0.430, Val loss 0.381\n",
      "Ep 4 (Step 000500): Train loss 0.397, Val loss 0.376\n",
      "Training accuracy: 90.00% | Validation accuracy: 82.50%\n",
      "Ep 5 (Step 000550): Train loss 0.385, Val loss 0.385\n",
      "Ep 5 (Step 000600): Train loss 0.455, Val loss 0.367\n",
      "Training accuracy: 85.00% | Validation accuracy: 90.00%\n",
      "Training completed in 1.39 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Run training loop (p 197)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaQZJREFUeJztnQdYFGcTx//0Dooo9t7FBtg19h6jxt7jZ28xapqxt2hMNPYeY4nd2GLvvaKiInYsgKJioXfue+Y9Dw8E5OCO3bub3/Pcw+7d3u7c3rGz77wz8zdRKBQKMAzDMAwjS0ylNoBhGIZhmLRhR80wDMMwMoYdNcMwDMPIGHbUDMMwDCNj2FEzDMMwjIxhR80wDMMwMoYdNcMwDMPIGHbUDMMwDCNj2FEzDMMwjIxhR80wTKZp0KABvvvuO6nNYBiDhh01w0jIN998AxMTk08eLVq0kNo0hmFkgrnUBjCMsUNO+e+//072nJWVlWT2MAwjL3hEzTASQ045b968yR45c+YUr508eRKWlpY4c+ZM0vazZ89Gnjx58PLlS7F+8OBB1K1bFzly5ECuXLnw5Zdf4tGjR0nbP3nyRIzSt27dinr16sHGxgbVqlXD/fv3ceXKFXh6esLe3h4tW7bE69evk43227VrhylTpiB37txwdHTE4MGDERsbm+ZniYmJwffff48CBQrAzs4ONWrUEJ9BxdOnT9GmTRvx+ej1ChUqYP/+/Wnub8mSJShVqhSsra3h6uqKjh07Jr2WmJiImTNnolixYuIzVa5cGdu3b0/2fh8fH/G56PPR+3v16oXg4OBkoftvv/0WP/74I5ydncW5nzx5coa+N4bJLthRM4wezAGTgwkJCcH169cxYcIErFq1SjgeIiIiAqNHj4aXlxeOHTsGU1NTtG/fXjgydSZNmoTx48fj2rVrMDc3R/fu3YWDmj9/vrgRePjwISZOnJjsPbS/O3fuCGe7adMm7NixQzjutBg+fDguXLiAzZs34+bNm+jUqZOIGDx48EC8PmzYMOHMT58+jVu3buG3334TTjQ16POQE506dSru3bsnbki++OKLpNfJSa9btw7Lli3D7du3MWrUKPTs2ROnTp0Sr79//x6NGjVC1apVxb7o/XRz07lz52THWbt2rbhpuHTpkrgJouMdOXJE4++KYXQGyVwyDCMNffr0UZiZmSns7OySPWbMmJG0TUxMjKJKlSqKzp07K8qXL68YMGBAuvt8/fo1Sdcqbt26JdYfP34s1letWpW0zaZNm8Rzx44dS3pu5syZijJlyiSzzdnZWREREZH03NKlSxX29vaKhIQEsV6/fn3FyJEjxfLTp0/FZwkMDExmT+PGjRVjx44VyxUrVlRMnjw5Q+fm33//VTg6OipCQ0M/eS06Olpha2urOH/+fLLn+/Xrp+jWrZtYnjZtmqJZs2bJXvf39xef+969e0n2161bN9k21apVU/z0008ZspFhsgOeo2YYiWnYsCGWLl2a7DkKw6qg0PeGDRtQqVIlFClSBH/++WeybWm0SiNhGhFSWFc1kn727Bnc3NyStqP3q1CNxitWrJjsuVevXiXbN4WTbW1tk9Zr1aqF8PBw+Pv7C1vUoRFyQkICSpcunex5GkFTSJ6gEfKQIUNw+PBhNGnSBB06dEhmlzpNmzYVxyhevLgYldODIgVkD43+IyMjxTbqUFieRtDEjRs3cOLEiVRH7DQ1oLIz5fHz5cv3yXlgGClhR80wEkNh15IlS6a7zfnz58Xft2/fige9RwXN+ZJDW7lyJfLnzy8cNTnolHPJFhYWScs0Z53acynD5ZpADtzMzAxXr14Vf9VROcv+/fujefPm2Ldvn3DWFL6eM2cORowY8cn+HBwcRJiewu60Ld2M0PwxzavTsQjaD82Hp5aIR9vQuaHwekrIGad2XrRxHhhG27CjZhiZQ6M/mn8lR7xlyxb06dMHR48eFXPRb968EfO39BolihFnz57V2rFpVBoVFSWStYiLFy8Kp1uoUKFPtqWRLI2oaTSqsiU16L2UlEaPsWPHCttTc9QEzaXTyJseNMdOCXPHjx8XI2lyyBQ1qF+/fqrvdXd3x7///ouiRYuK/TCMvsK/XoaRGAoNBwUFJXuOHIuLi4twfJQgRaPQvn37ivAvhatpFPrDDz+I7GkKK69YsUKMEslx/fzzz1qzjUbl/fr1E0lolD1OzpISxugmISUUSu7Rowd69+4t7CPHTVnklJBG4eXWrVuLxDjKwqZt3717J0LT5cqVS/XYe/fuhZ+fn0ggo89J2eE00i1TpowYbVN2Od3A0HOU9U7JdufOnRPZ6XQzQ4lrdBPQrVu3pKxuCplTohsl46Uc9TOMXGFHzTASQ9nI6qFYgpzR3bt3MWPGDFHSRE6LoO3IKZPzadasmZhDJsdDc78U7qb3LViwQGSLa4PGjRuL8ihylnRDQcdNr3yJ6sGnT5+OMWPGIDAwUNxs1KxZU5SMEXTjQQ40ICBAOFS68Ug5566CRs+UZU7Hi46OFnZQ5jmVdBHTpk0TZWMUPieHTtvTKPqXX34Rr9M0ADnun376SZwrsp+mCOiYqd1oMIxcMaGMMqmNYBhGflAdNZU47dq1S2pTGMao4dtKhmEYhpEx7KgZhmEYRsZw6JthGIZhZAyPqBmGYRhGxrCjZhiGYRgZw46aYRiGYWQMO2odsnjxYtEViST6SO7v8uXLMDRIBYnaNFLNKrVeTFnKQykQ1PqR6n+puxV1mFIpKamglpjUKIPqaqkWlhpsqFpEqiAlJup2ReeSOluRypE+QDW+JClJDTpImpJkI6mTmDpUI0y1xdS4hLp+Uf9rlYSlCmpkQg1DqM817YeancTHxyfbhlptUh0xdeyilqRr1qyBnKH+5tQIhb53elAf8QMHDsDYz0tazJo1S/yPUdMYFcZ8jiZPnizOh/qjbNmyhnluskX6wwjZvHmzwtLSUrF69WrF7du3heJRjhw5FC9fvlQYEvv371eMGzdOsWPHDqFKtHPnzmSvz5o1S+Hk5KTYtWuX4saNG4qvvvpKUaxYMUVUVFTSNi1atFBUrlxZcfHiRcWZM2cUJUuWTFJAIkJCQhSurq6KHj16KHx8fITyk42NjWL58uUKudO8eXPF33//Lez29vZWtGrVSlG4cGFFeHh40jaDBw9WFCpUSChZeXl5KWrWrKmoXbt20uvx8fEKNzc3RZMmTRTXr18X59zFxSVJkYrw8/MTalKjR49W+Pr6KhYuXCiUrA4ePKiQK3v27FHs27dPcf/+faFm9csvvygsLCzEuTLm85Ialy9fVhQtWlRRqVKlJLUyYz9HkyZNUlSoUEHx4sWLpAcpxxniuWFHrSOqV6+uGDZsWNI6yQLmz59fSAkaKikddWJioiJv3ryK33//Pem59+/fK6ysrISzJejHT++7cuVK0jYHDhxQmJiYJMklLlmyRJEzZ04h96iCZAjVJRn1hVevXonPe+rUqaTzQc5p27ZtSdvcuXNHbHPhwgWxThcQU1NTRVBQUDK5SZKAVJ2TH3/8UVy01OnSpYu4UdAn6HsmOU4+Lx8JCwtTlCpVSnHkyJFksqLGfo4mTZokbvBTw9DODYe+dQD1RyYFIQrzqqCWhbR+4cIFGAuPHz8WPazVz4OTk5OYBlCdB/pL4W5PT8+kbWh7Ol8k26jahlpYktyjCup9TSFk6hetT1A/anUZS/qdxMXFJTtHFL4rXLhwsnNE/b1V0pSqzx8aGorbt28nbaO+D9U2+vJ7o9ai1Ao1IiJChMD5vHyEwrcUnk35OfgcQUyj0bQbSaHS9BmFsg3x3LCj1gGkCUwXHvUfAEHrKcUXDBnVZ03vPNBfmhtKKUhBjkx9m9T2oX4MfYDEI2h+sU6dOkk60WQ/3YDQzUp65+hznz+tbeiiQ+pXcoU0rGn+kOb/SE1r586dKF++vNGfFxV080JSn5TrkBJjP0c1atQQ88XUK5/yHWhgQHksYWFhBnduWJSDYbJxZOTj46NVGUp9h0REvL29RaRh+/btQvXq1KlTUpslC/z9/TFy5EgcOXJEJFEyySEVNhWUlEiOm0RXtm7dmiTLaijwiFoHkGIQSeilzDCk9bx588JYUH3W9M4D/SX9YnUo65IywdW3SW0f6seQOyQNSQpYJOtYsGDBpOfJfpoqIfGL9M7R5z5/WttQNrWcL1o06qFMWg8PDzFqJDWw+fPnG/15UYVv6X+DMo4pykQPuokhdTRappGdsZ8jdWj0TPKpJGVqaL8fdtQ6uvjQhYd0eNXDnrRO82/GQrFixcQPXf08UMiI5p5V54H+0j8TXZRUHD9+XJwvukNWbUNlYDTnpIJGGTQaI51iOUM5duSkKaRLn4vOiTr0O7GwsEh2jmjuneba1M8RhYjVb2jo89PFgsLEqm3U96HaRt9+b/S9kxwlnxelxCh9Poo4qB6Uy0FzsaplYz9H6lBJ56NHj0QpqMH9frI1dc3IyrMou3nNmjUis3ngwIGiPEs9w9AQoIxUKm2gB/2c5s6dK5afPn2aVJ5Fn3v37t2KmzdvKtq2bZtqeVbVqlUVly5dUpw9e1ZkuKqXZ1EGJ5Vn9erVS5Tu0Lmlkgl9KM8aMmSIKE87efJksjKSyMjIZGUkVLJ1/PhxUUZSq1Yt8UhZRtKsWTNR4kWlIblz5061jOSHH34Q2a2LFy+WfYnNzz//LLLfHz9+LH4btE7Z/ocPHzbq85Ie6lnfxn6OxowZI/6v6Pdz7tw5UWZF5VVUWWFo54YdtQ6hmjv6oVA9NZVrUZ2woXHixAnhoFM++vTpk1SiNWHCBOFo6calcePGomZWnTdv3gjHbG9vL0oj+vbtK24A1KEa7Lp164p9FChQQNwA6AOpnRt6UG21CrppGTp0qChNootC+/bthTNX58mTJ4qWLVuK+nG6GNFFKi4u7pPvokqVKuL3Vrx48WTHkCP/+9//FEWKFBH20gWSfhsqJ23M50UTR23M56hLly6KfPnyCZvpmkDrDx8+NMhzw+pZDMMwDCNjeI6aYRiGYWQMO2qGYRiGkTHsqBmGYRhGxrCjZhiGYRgZw46aYRiGYWQMO2qGYRiGkTHsqHUMdVkigXP6yySHz0368PlJHz4/acPnxrDOD9dR6xhqmUnSjiQ6QK3pmI/wuUkfPj/pw+cnbfjcGNb54RE1wzAMw8gYdtQMwzAMI2NYjzoVSGbx+vXrQkbO1DRr9zIkYk4EBgaKcAvzET436cPnJ334/KQNnxv5nx9SiiPJzKpVqwrZ0vTgOepUuHLlCqpXry61GQzDMIyBc/nyZVSrVi3dbXhEnQo0kladQNI2ZRiGYRht8uLFCzEgVPmb9GBHnQqqcDc56YIFC0ptDsMwDGOgZGR6lZPJGIZhGEbGyMJRL168GEWLFoW1tTVq1KghQs5pERcXh6lTp6JEiRJi+8qVK+PgwYNZ2ifDMAzDyBXJHfWWLVswevRoTJo0CdeuXROOt3nz5nj16lWq248fPx7Lly/HwoUL4evri8GDB6N9+/YiSzuz+2QYhmEYuSJ51jeNdinjbdGiRUkp64UKFcKIESPw888/f7J9/vz5MW7cOAwbNizpuQ4dOsDGxgb//PNPpvaZkoCAALG9v78/z1HrOQkJCSIKwzCGhIWFBczMzKQ2g8kCmvgZSZPJYmNjcfXqVYwdOzbZxHqTJk1w4cKFVN9DvVkpnK0OOemzZ89mep+M4UH3n0FBQXj//r3UpjCMTrBzcES8lROK57aX2hRGx0jqqIODg8WIJ2V6Oq3fvXs31fdQCHvu3Ln44osvxDz1sWPHsGPHDrGfzO6TnL96c3ZVMbxWePcU8PoLaDwJMOU74OxC5aTz5MkDW1tbmJiYSG0Sw2jtJjQsPBz3ngRil+8D1HYriQ4eHPkzZPSuPGv+/PkYMGAAypYtKy6+5Kz79u2L1atXZ3qfM2fOxJQpU6B14mOBv5oC4S+BXKUA917aPwbzCXSjpnLSuXLlktochtEqiQoFXoQnwCFHLjQuHo+f9/igqIsdPIrklNo0xhCTyVxcXMQ8C7VRU4fW8+bNm+p7cufOjV27diEiIgJPnz4Vo2R7e3sUL1480/ukMDmpqKgelKSmFcwtgdojlMsnZgCxkdrZL5MuqjlpGkkzjKHxIiQa4THxMLWwgr2VOewsgEHrr+L5+yipTTNcFArgyCTg9X3jc9SWlpbw8PAQ4WsVlPhF67Vq1Ur3vTRPXaBAAdGX+99//0Xbtm0zvU8rKyshdaZ6ODg4aO0zovpAIEdhIOwFcHGx9vbLfBYOdzOGxpvwGPEg8uWwQS57S5TIbYfg8BgMXO+FqFjlFCCjZXx3AefmAauaADHhMLryLCqjWrlyJdauXYs7d+5gyJAhYrRM4Wyid+/eyRLDLl26JOak/fz8cObMGbRo0UI44h9//DHD+8xWzK2U89PE2flA+Ovst4FhGL2HRtHP30eLZVdHazhYW8DUxATT2rrB2c4SPoGh+H77DTGHzWgZVzegTGug5hDAyt74HHWXLl3wxx9/YOLEiahSpQq8vb1FAxNVMtizZ89ET1QV0dHRopa6fPnyon6aRtWU8Z0jR44M7zPbqfA1kK8KEBsGnPpNGhsYo4Sa/sybNy/D2588eVJEIjhbXl7Exifg2ZsIKKBADhsL5HGwSnqNRtZLe7jD3NQE+26+wMLjDyW11SBxKQV02wjU/8k466jliE7qqB+fAdZ+CZiaA0MvAS4ltbNf5hPoZu7x48coVqzYJ6V8+hqmp+Y9kydP1ni/r1+/hp2dXYbn66m88e3bt+KmlqcO5EFCogKPXocjOi4BNhZmKJHbHqamJp/8zjddfoaxO26J9yzr6Y4WbiwolGXIPero/0ATPyP5iNpoKFYPKN0CSIwHjn4IhTPMByhqpHrQCJhyJdSf+/7775O2pXtrys3ICJR8qUlSHeV4UNKlMTppukmRG/Rd+7+NFE7a3MwURXLZCSedGt2qF8Y3tYuK5VFbbsD3OetQZ5nj04Hdw4HQj1FdKWBHnZ00mQKYmAJ39wLPLkptDSMjyDmqHk5OTsJRqtapsoESHA8cOCASJSn5kaZ7Hj16JJIoafRLlQ/Uje/o0aPphr5pv6tWrRLTRuTAS5UqhT179qQZ+l6zZo2YVjp06BDKlSsnjkN5IerTUXTT8O2334rtqBzup59+Qp8+fdCuXbs0P++bN2/QrVs3MXVFdlSsWBGbNm1Ktg3lnsyePRslS5YUn7lw4cKYMWNGshEJ7cPZ2VlEDTw9PUUOC/HNN998cvzvvvsODRo0SFqn5eHDh4vnqVqEejQQ1KeB7KF90ohn6NChCA9PnkB07tw58X6yPWfOnOK97969w7p168Q5UO/LQJAtvXppXp75MjQaodFx4jsp4mwLS/P0L9njW5dD3ZIuiIpLwIB1XiLJjMkkIQHAhUXA9fXAC29ICTvq7CRPWaDqh3/WwxOUYRUm20YmkbHx2f7Q5swStb+dNWuWSJCsVKmScB6tWrUSFQ3U654caJs2bUReR3pQz4DOnTvj5s2b4v09evQQ4e60iIyMFDkf69evx+nTp8X+1Uf4v/32GzZs2IC///5bOLDQ0FBRQpkeFLalm459+/bBx8cHAwcOFI5MXTyHkkjp806YMEGUTG7cuDEpz4Q+e/369REYGChuNG7cuCESSsm5awIlnFIUgexetmxZUifDBQsW4Pbt2+L148ePJ0tWpZyXxo0bizwZ6nZIN0103ql+v1OnTuKv+s0PaQzQ5/zf//6nkW3vI2PxKkzpaAvmsIGd1efbXtCoe1H3qiiayxaB76Mw5J+riI3X7JwwaqPp+GigSF1lNFRC9K7hid7T8Bfg1jYg4DLguxuokPaog9EeNMIoP/FQth/Xd2pz2Fpq59+MVOOaNm2atE4jSRKcUTFt2jTs3LlTOAkaKaYFjTZpJEr8+uuvwimRgyRHn1ZdOjkxai5E0L7JFhUkkENOlUbpBPXY379/f7qfhUbS6s6e+vDTqH3r1q2oXr266A5IzY1oXzQ6J+j4devWFcvktGn+/cqVK+I8EDTy1hSKKNCoXR0aYatHJKZPny7Ef5YsWSKeo+1p9K5aJypUqJC03L17d3HTQk6bIA0Cigaoj+Y/B93kBbxT1kXndrBCTjvLDL83h60lVvWphvaLz+HKk3eYsMsHszpUNMrpjEzz4iZwY7NyudlUnc1TZxQeUWc3Dnk/NkE5OhlIYMEIJmOQc1CHRpXk7CgkTWFnCkvTaPtzI2oajaug8C7Nh6enLEfhXZWTJvLly5e0PTUIomZC5FxVUMMhGi2nB4066caCQszkaMl2ctQq2+lzUPiYRq6pQaPaqlWrJjnpzJKanTR9QMelmwmacqCRPoXqKbKgOnZadhHUOfHw4cNitK+aPqCbo4w6yriERDx9Eyk6kDlaWyCvo+YJkSXz2GNB96qg6ewtXv74+9wTjfdh1ByZSHE4wK0DUCD933J2wCNqKaj9LRBwBag1HDCzkNoao4CyZWl0K8VxtQU5VXXISR85ckSEpWk0SeI0HTt2/GxSFCkvqUMOJL2QcWrbZzWk//vvv4sRM82fq+aDaSSrsp0+S3p87nUKX6e0MTUVtZTn9MmTJ/jyyy9F7wWaD6cbAQpt9+vXT9hGNy2fOzbdQFCkg+armzVrJkLoFPrOCImJCuGkyVlbmZuhkLNNpkfCDcvkwdiW5TBj/x1M3+crnPcXpXNnal9GxcOjgN8JwNQCaEwOW3p4RC0FVDDfaydQMu27cka70MWOQtDZ/dBluJHmVWmkRiFncnaUeEaOJjuhxDeaN6YQtPpomXTgP2c7JcL17NlTODVqAXz//v1kIWlyiOodBlNGBWhkm9bcOmW7qye8EbT95yDlPbppmTNnDmrWrInSpUvj+fPnnxw7LbtU9O/fX4ykKQROyn2UlPY56MYi4H2UCHubmZqIeWYz06xdovvXK4aOHgWRqACGb7wGv9fZ31VLr0hMULYKVXWVzKnMopcadtRyIJ4zMxnNIWdGXfrIAVEyFc2NappMpQ1ofpmEbXbv3o179+5h5MiRIgM6vZsUsp2iAefPnxdh7kGDBiXrz091wZQ9TklcNDKlDPeLFy/ir7/+Eq/THDvdmFA2NTl96lRIrYRVUraNGjWCl5eXeO+DBw9EHTolrX0OikzQyJvm3WmflECnSjJTQfPxdGNC2eCUkEdZ+UuXLhXKfSrou6CsdOqQmNEkstfhMSKBzATKDG8rLURj6DuY0d4N7oVzIDQ6Hv3XeiEkiqfb0oTmpV/6ANZOwBcfcyikhh21lCTEA6f/AP6soCwFYBgNoDIiKg2qXbu2yDqmEiF3d/dst4McKjlOavdL/fRpvplsSa/ZDHUXJFtpO0qyUjlddSjbe8yYMaLDIM3DU8dB1dw4ZWrTPDAppFHmOkUUKEOc5scJ2i+9nxw9la1RchrZ9zlodE/nlTLZ3dzcRDY73YSoQ6NsOjbdHNHcPH1mukkxNzdPFmno0KGDOBfplampCI2KQ1CIsj1o/hzWsLfW3pQYhdCX9/JEfidr+AVHYMSm64hP4EzwTyDRJMr0Jup9D9hmLf9Bm3BnsuzqTJYadOr/bgU8Ow988SPQaJzujmVE6GNnMkOCRvXkWKkEjBLGjBVKOKNscMqqTw9qZvLwVbhIHstlZ4kCOW118jv3CQxBp2UXRAVEv7rFMOHL8hn+LEbBmTnAsamAU2Fg+BXAwlo2foaTyaSEQoMtfgWCHwBuHaW2hmEyBcnN0giT6popU5tKqsiBUPjXGKGwPzWOoYd6CVdq0Mj2yZsI4aSpTpr6dusKtwJOmNO5MoZuuIa/zj5GGVcHdK72+blzoyAiGDjzp3K58QSdO2lNYUctNfmrKh8Mo6dQhjUlTlEWOgXoKGRMJU40qjZGKOubnDWFz8uUKZPmduScn76NFA1JqOMYzUuTGpYuaVUxH0Y2LoX5xx5g3K5bKJ7bDp5F5RPilXRuOjYMyFdZloMmdtRyIjYCCAsCcn2sWWUYuUPhO0roYpRkNPP+xfsoRMTEC+dcNJed6CqWHZCjvv8yDAd8gjD4n6vYPbwuCuhwJK8X1BqmzPC2y013npAb8rPIWPG/DCxwB7b2VpYIMAxjsLwJj8GbCGXNeGFnW1hrsd7+c5CoB4XAy+dzRHB4rMgEp5Iwo8bEBCj3JVC4BuQIO2q5kKskEBelLA24uUVqaxiG0RHh0XF4/l6Z4Z3XyRqONtnf9Ihq/Ff28YSLvSXuvAjFmK03RLMVo+PNIyBK/trr7KjlApUC1ButXKYSAXLaDMMYFDHxCWJeWgGF6Mmd295KMlso3L2spwcszExEGJzmrY0KhQLYORhYUAV4dAJyhh21nKgxGHAqBIQGAhfTzxZlGEa/SEhMxNPgSCQkKmBraSYUsaQWyqBEshntK4plctT7bkqru5ytRLwGYkKVDadyl4WcYUctJ6gkoNEE5TKVClDJAMMweg9lw/u/jUJ0fAIszExRJJedmCuWA509C4m6amLMNm9Rb20U2OcBBp8D+u4HHPNBzrCjlhsVOwF5KylLBU4ll99jGEY/CQqNRmh0nMjwLpLLVjhrOTG2ZVkh2BEdl4iB67zw+oMOtsFjZq4X5bHy+rUwytKAZh+6OXn9pUx2YJgMQu04U+opk0JVelD4ddeuXVk+trb2Y2i8i4xNcnwFc9poTZ9cm1Bp2MJuVUVd9fOQaAxa7yXm0w2S6FDgwhIgTpnQpw+wo5YjxRsAJZsCifFKzWrG4KFe3S1atEj1tTNnzggnSAIQmkLiEQMHDoQ2mTx5MqpUqfLJ86RW1bJlS60eS9+JjIlHwDtlYmgeByuRQCZXnGwssKq3JxytzXHt2XuM2+mTZTlTWXJuHnBoLLBZfzrnsaOWK02nACamwJ09yhprxqAhvWNSk6L+vykhqURPT08hr6gpJPdIGsrZAQlrWFlJl8UsFWnpf8fFJyozvBUKOFpbwNVRXm0pU6N4bnss6u4Omj7ffjVAtBo1KEICgQuLlcvV+kFfYEctV1wrAFU+3PEdHq8sJWAMli+//FI4VWrFqU54eDi2bdsmHPmbN2+ESlWBAgWE8yXFqE2bNqW735Shb5J8/OKLL4SIQ/ny5cXNQWpqWKQQRccgnWhSoSLpR4LsmzJlilCOolE+PVQ2pwx937p1S8hNkq50rly5xMiePo8K0tImZak//vgD+fLlE9sMGzYs6VipQXKXpGNNGtikTEXKWNSuVB3qN06fgTqm0Y0DSVeq5DGJ27dvi/Pt6OgIBwcH1KtXT+w3takDgmwkW9XPKYmNkBoX7UMVsUh53r778WdERseIZiaFnG3F+fnvv/+EzXT+XVxchJY4MXXqVNF6NSUUuaDzn53QXPX41krBjl/338GJe0rFMoPgxAwgPhooXBso0wr6AjtqOdNwHGBuA/hfAu78J7U1htOmVdMHyZGqoGV6LmWde2rv0wCSSKQLPzk99XAjOemEhAThoEktycPDA/v27RPayuQgevXqhcuXL2dY1errr78WEpGXLl0SOsvkXFJCzovs8PX1xfz584Wm8p9/KgULSGqSpCdJFYpC3fSg51ISEREhpCZJhpPC7/Q5yKEOHz482XYnTpwQTpL+rl27Vhw35c2KOuToSdby2LFjuH79upguoGmDZ8+eJW1D55FuYEi1irSuly9fLpw6ERgYKG5UyIEfP34cV69eFXrR8fGadeaimwuSxCQbVI5Udd7oRmD8tNnY8s9abPxrKYrmsoWZqYn43sgxk/30PvoMJJNJkA1kK50rFbQNTXf07dsX2U3fOkXRxbMQqAfKtxuvC3UvvSfIB/DeqFymPCCJS+M0gmQumeT4+/vTlVL8lZxj0xSKWUUVihtbpbZEb4iKilL4+vqKv58wyVHzh8+Oj++nZXpudavk+/2t2Kfv05A7d+6I392JEyeSnqtXr56iZ8+eab6ndevWijFjxiSt169fXzFy5Mik9SJFiij+/PNPsXzo0CGFubm5IjAwMOn1AwcOiGPu3LkzzWP8/vvvCg8Pj6T1SZMmKSpXrvzJdur7WbFihSJnzpyK8PDwpNf37dunMDU1VQQFBYn1Pn36CPvi4+OTtunUqZOiS5cuCk2oUKGCYuHChWL53r17wo4jR46kuu3YsWMVxYoVU8TGxqb6esrzR7Rt21bYqoJsbteuXZr2vAyJUtzwf6cYPW6aoqq7e9LztWrVUvTo0SPN97Vs2VIxZMiQpPURI0YoGjRokLnfuRaIiUtQdFx6TlHkp72KBr+fULyPSP2c6Q3r2iv/L7d+/C71xc/wiFru1PkOGOkNVOoktSWMjilbtixq166N1atXi/WHDx+KRDIKexM0sqaQK4W8nZ2dxSjx0KFDyUaT6UEjNgoH58+fP+m5WrVqfbLdli1bUKdOHTHnTMcYP358ho+hfiwacdrZ2SU9R/ukUf29e/eSnqORuZnZxz7XFAJ/9epVuiNqUukiZa4cOXII++hYKvu8vb3F/khyMzXodQp1W1hkrW0n5Qykdt5q1qqN8iWLoGaZglj8x3QE+PsnOzZpVKfFgAEDRCSAIic0771x40Yx0pYKUvRa2tNDdDB7HByBYRuvCVlOveTRceDRMcDUAmg8EfqG/OoEmORYKUN2jJb45bnm7zFTS5Aq20a5D0r0U+e7W1pLKhsxYgQWL14skshKlCiR5HR+//13EYqmOWdy1uQEaT41rWSmzHDhwgX06NFDzENT6NrJyQmbN2/GnDlzoAtSOkyaxyVnnhbkpGlenULPNPdM898dO3ZMOge0nh6fe50kO1NmOqc2Z65+A6J+3oaOGYuR42egYF4XnNy/K9l5+9yxKYRPIfmdO3eK6Qk6Ln02KXGxt8LK3p7ouOw8zj4MxvR9dzD5qwrQKxITgcMfnHO1/oBzcegbPKLWF+ji4bsHuLRCakv0G0s7zR/UFEEFLdNzFikuuqm9LxN07txZOAsaTa1bt06MqFRtJklKkhKpevbsKUarlLB0//79DO+bRqH+/v5iXlnFxYsXk21z/vx5FClSBOPGjROjxlKlSuHp06fJP6qlpRjdf+5YlHBGc9UqyH76bOlpNH8O2gcldtFcL92s0KhfXVaSniNHf+rUqVTfT5nzFKVIK2GNEvrUzw99TsoH+Bxnz55D/oKF0H/EGNSoXg113N0+OW90bJqXTi9PoU+fPuIGjR5du3b9rHPPDsrnd8TczspyvDXnn2DzZc2iK5Jzcwvw8hZg5QTU/xH6CDtqfeHxKWBrL+DIRCA0E6NCRi+gUC4lZ40dO1Y4DPVsY3KaNJokZ0rh3kGDBuHly5cZ3neTJk1EVjI5A3Ki5LDIIatDx6AwMo2iKcmLErJohKcOZT0/fvxYhHKDg4NFlnVKaHRJmc10LHJ0lCxGkQJKfqOM7cxC9u3YsUMcmz5D9+7dk43AyTY6Jt3gUAY62Xny5Els3bpVvE7JbKGhocIJenl5iSz49evXJ4XjKUudkr7ocffuXQwZMgTv36evrpRI5Vd5C+FFYACO7N2BuHcvsGjhwk/O26RJk0Rom/7S90dZ8b/99luybfr37y+S3A4ePChp2DslLdzyYnTT0mJ5wm4fXPJ7A70gLkopckSQ6BGJH+kh7Kj1hWL1gRKNgNojACsHqa1hdAiFv9+9eydCz+rzyTRX7O7uLp6nMiIaTVLpUEah0Sw5j6ioKJFtTE5hxowZybb56quvMGrUKOHQqDSIbgpSlgd16NBBZFs3bNhQjEBTKxGjEiWaP3/79q0oR6IQLs3PLlq0CFlh7ty5IpOc5vIpVEzngs6JOkuXLhXHGzp0qJj3p7lf1cieSsDIEdJcN00pUBY9ZbWrQvDkHMnRU+Y4vU5RC/qcaUFh8ufvo1CrYXP06j8UM8f/CE8P91TPG31nlP2+Z88ecW7ppiBlxj7diNBnI7tr1JCXNvKIRiXRulI+xCUoMGTDNfi/jYTsubQMCA1Qih2R6JGeYkIZZVIbITeo6QQl3VCYsGDBgpAN9FXpU0mBRFAyDo2kihUrJkZ1DKMrgsNjhKMmiuayy7K2NF2OyVnTTcbo0R9kb2X0O4+KTUCn5efhExiKsnkdsH1IbdhbyTTVKeKNUsKSFLLaLwcqd4W++hkeUesT6k6a768YRlLCo+Pw4r2yX3Q+J+ssO+nXr1+LiENQUJAktdMZwcbSDCt6eYoks7tBYRi1xRuJVGwtRyxtleHuovWAip2hz7Cj1keengdWNZa92DnDGCoxcQnK9qBQIKetpXBcWSVPnjyiQ9mKFStEeF+u5M9hgxW9PWBpZoojvi8x90jGExqzFQsboO4ooM9/SrEjPUa/rTdWfHcDgVeViWXplLIwDKN9EhIT8eRNJBISFUIJi+qMVZn5WQ1706iaEuTkjnvhnJj5dUWxvOjEQ+y5IbMEV4XaKN8Apgsld9RUL0qZmjTHQskTn2uHSDWkVN5BZQsU36fEF5qrUS+noCQOmrehbagOlZpEGNRU/Bc/AlaOQNBN4JYym5VhGN1D15Fnb6OEBCRpSpO2tCkpWBghHTwKYtAXyprkH7bdwM2A9LPjsw3/K8DSOsCD5D3g9RlJHTV18qGECSpXuHbtmqgNpSzOtDoTUW3pzz//nFTeQI32aR+//PJL0jZU7kBZnzTXQ9vQ+uzZs7Fw4cJs/GQ6xi6XMqRDHJumV7qqDKPPBIVEIyw6DqYmJsJJk7M2Zn5sURaNyuZBTHwiBq67ilehMrgWnf4deHUb8E1eHqfPSPoro1ILKp2gxAlS8iGRACrrULVQTAmVPFAbQgoN0Si8WbNmQqxAfRRO21BTiNatW4ttqEyDtsuocIHeUHMI4FhQWXpAJQjMJ6TX4YphNOVtRCxehytrxgvmtBFhb2P/fZPYyPyuVVAyjz2CQqMxYP1VRMel3wxH53y9HKj9LdDg4wBO35Hsl0Yt/0i5hho7qNd5UlMGaseXGlRf+M8//winS3Wgfn5+2L9/v2iioL4NJWNQxyZq7kBNEc6ePStuCgwKSpRoNB7YNRg4Mxdw7623xfzahjpn0W/p+fPnos6X1rUxh8gYL5Gx8Qh4FyVC3852VrA2TUw25ZadkA10/aT5bPqd0+9bShysLbCqtyfaLj6HG/7vMXbHLcztXFm6/zmbnEp1LANCMkdNHY1oPjlllyJap45AqUEjaXpf3bp1xY+VpOkGDx6cLPRNoXHqPEQNA6g5Px2DmjpQp6S0oM5K6t2VwsLCoBdU6qIUQaf2eBTuaTFTaotkAV28KEeBOnuRs2aYrEBJY6/CYsRfGwtTWERaITxYaquUTWUKFy4sfu9SU9TFDkt7uKPX6svYeT0QZfI6YHD9EtlrREgA4FjAIJLHUiLTSvXUoVaAv/76K5YsWSISz0hdaOTIkSJZTNUFiFoFbtiwQcxnkzIPtRok4QLq8EQdh1Jj5syZQoRA76B/0GZTgfXtgcsrgeoD9LLhvC6gUQZdxOhm7nN9qRkmLaJi4/HdFm+hx1zMxQ4LulWVPORN0CCEeoPLKVJUu6QLJrUpj4m7b+O3g3dRKo89GpfLfLtYjYgOBZbXB3KXBTr+BTjkhSEhWWcyCt3QHeH27duTtUEkZ0q9dXfv3v3Je0iermbNmkJFSAWFwgcOHChaAtKdJWWC06h62LBhSdtMnz5dbJfWSD3liJrE5WnOXHadydJi/ddKCbcK7YFOa6S2hmEMAro0Dt94HftuvUAuO0vsHl4HBXPaSm2W7M/ZuF0+2HjpmehYtmNobZR2zYaWx8enK6OKziWAYZcAs6w1n8kO9KIzGY14qM+uupoMJUfQemoauURkZOQnYR6Vlq3qfiOtbdJLvCBpOUdHx6SHg4Oe9dJuOpXuuYDbO4EAL6mtYRiDYMGxh8JJW5iZYFkvD3bSGYBG+FO+qoAaxZwRHhOP/mu98C5CezKsqUIiRec/9JBvMlkvnLSmSDq5QaVZ1BB/7dq1opSKlGqoeb6qfR41xldPNqMm/FR6Rco+1OOWlIQo5E3Pqxw2LdOcNKnfkPwdiRBQIhnJ4hksed2AKh+aJByewO1FGSaLHLj1An8eVXbcmt7ODdWKcqJmRqGStaU9PVDI2QbP3kZi6IZriEvQYYb6iRlAfBRQqCZQrg0MEUknW0jOjzIXJ06cKPrbkqIMybupEsxIbk99dEzqQXTHRn8pPE0ZvSrHrILqpcl5U1N7qsemuWmSA6RjGDQNxwE+O4CAy8ArX8BVz8TdGUYm3H4egtFbb4jlvnWKoku1wlKbpHc421liVe9q+HrJOVzwe4Op//liWjs37R/o5W3Ae6NymTK9ZTRnr01YPUuf1LM+x63tQL4qgEtJqS1hGL1Vw2q76BwC30ehXikX/P1NNZgbeVOTrEC9wAeu9xJBPopM9KxZRLsH+Kcj8PAIUO4roMt66BN6MUfN6ICKHdlJM0wmobagg9dfFU66uIsdFnVzZyedRZqWd8X3zcqI5cl7buPCozfa27nfSaWTNjVXzk0bMPwrNFSCfIAYPakHZxiJocDihF0+8Hr6Dg7W5ljZxxNOtoaXlCQFQxuUQNsq+RGfqMCQDVfx7E1k1neamKjMxyE8+wG5srlmO5thR22InPgVWFYXOLdAaksYRi9Yfe4JtnoFgPQ1FnV3R4nc9lKbZDBQXtFvHSqhckEnvI+MQ/91V0S/9Cxxa5tSlIjEier/CEOHHXU2tB7MdmF1kUimAEIDs/e4DKOHnLr/GjP2+YrlX1qVQ/3SuaU2yeCwtjDD8l6eyONghfsvwzFqi7fo9JYp4qKB4x9ahNb9DrBzgaGjsaMmoQsSN6eMbCZ9/N9Gov3i80llHtkGJVYMPgu0W5K9x2UYPePR63AM33gN5DM6eRREv7rFpDbJYMnrZI0VvT1haW6Ko3de4Y/D9zK3o8vLgRB/ZbvQmkNhDGjsqKkd544dO1C8eHE0bdpU1DSrd/ViPnL16TvcexmGhccf4r/sFFanEoW8SlF3hmFSJyQyDgPWeiEsOh4eRXJiens3WbXkNESqFMqB3ztWEstLTz7CruuZiPqFBSkbPFFJKokTGQGZctTUP5sUrMqVK4cRI0YgX758GD58uNCUZj7SrmoBDPwgrP79thu4FRCS/Ua8f/axaw/DMIL4hEQM33QNfsERyO9kjWU9PWBlrmyaxOiWtlUKYEgDZfLXj//ehLf/e8120GImMPQiULkrjIVMz1G7u7tjwYIFQp1o0qRJWLVqFapVqyaalpCeNJdnK/mpRVk0LJNbCKsPWOeVvcLqUe+BJbWAw+OAx6ez77gMI3N+3X8XZx4Ew8bCTGR453awktoko+KHZmXQpFwexMYnYuA6LwSFaHhdzFMWMDWeG6tMO+q4uDihVPXVV19hzJgx8PT0FM66Q4cOQnYyPVlJY0IIq3ermiSsPjA7hdVtcqi1Fh2vLGlgGCNn6xV/rD73WCyTbnKF/E5Sm2R0mJqaYF7Xqijtai8kRKkpymevixeWAMEPYIxo7KgpvK0e7iYpSR8fH5w9e1b06Kb2nUePHhU9thkljh+E1Z1sLESY55cdt7Iv4lD/J2UJw4sbgM+/2XNMhpEpV568xbhdt8Tyd01KoWXFfFKbZLSQuha1Gc1pa4GbASH4cfvNtK+LAVeBQ2OBpbU/zFEbFxo7agpvP3jwQIhjUL/tP/74A2XLlk22TbFixdC1q/HMH2girE4j7B3XA7HitF/2HJhKF6iEgTg2VVnawDBGSMC7SNF5LC5BgVYV8+LbRqWkNsnoKZzLFkt6eMDc1AR7bjzHkpOPUt/Q2gko3QJw62hwWtM66fX99OlTFCmi5X6tRtTre92FJ0JYnZJL/+rjiUZls0FYPTYSWOgBhD0Hmk4D6nyr+2MyjIyIiIlHx2UXcOdFKMrnc8T2IbVgaympJhGjxj8Xn2L8Lh9xXVzRy1O0Hk2V+FjA3BKGgE57fZMi1aVLlz55np7z8mIt5M/Rq2YRdK9RWDSp/3aTNx68zIY2n5a2QKPxyuUzfwCRb3V/TIaRCdRwaMzWG8JJu9hbiuQxdtLygsQ66NpI18XvNl/H3aDQ1Dc0NwwnrSkaO+phw4aJO4CUUBicXmM0FFZflw3C6gSVMri6AdEhwJk5uj8ew8iEecce4ODtIFiamWJ5Lw8UyGEctbf6xsQ25VGreC5ExCag/1ovvKXr4tW1wH8jgbCXMGY0dtS+vr6iNCslVatWFa8xmgmrP30TiWEbdSysTlApQ9MpyuXLK4B3T3R7PIaRAftuvsCCY8pM4Rnt3eBRxFlqk5h0rotLerijSC5bBLyLwqh1Z6CgVqFX1wC+u2DMaOyorays8PLlp3c3L168gLk5h5M0FVa3szTD+UdvMG1vNtzklGwCFG8IJMQqE8sYxoDxCQzBmG3eYrl/3WLo5FlIapOYz5BTXBc9RUa4e+A/MIl4DYVzccCjL4wZjR11s2bNMHbsWISEfOyy9f79e1E7TS1FmYxTJq+DqCWkBIp1F55iw6Wnuj9oU3LQJspSrcCruj8ew0jAq7Bo0WAoOi5RiGyMbVVOapOYDFLK1QHL2uXDALN9Yv1UoaFGOzedaUdN5Vg0R02Z3w0bNhQPKscKCgrCnDk895kVYfVJu7UsrJ4a+Sp9bL13eCIJ8er2eAyTzcTEJ4gyrBch0Sie2w4LulUVZZGM/lDXfxVsTWJwLbEk+l3Jj3MPg2HMaOyoCxQogJs3b2L27NkoX748PDw8MH/+fNy6dUukmjNZE1Yfqi1h9fSgDPD87h/rqxnGQKBq0192+ODas/dwtDbHX32qiUZDjB7x6g5wfb1YPFvsO1D6ztAN1/AkOALGisZ11MaALuuo04La53VZfgE3AkJQxtUB/w6tLeZpGIbJOCtP+2HG/jtiBL2mbzXUK8Xa0nrHhs7Ag0NA2S8R3WEduq28iOvP3os2zDuG1hadHg0BndZRq6AM74MHD2LPnj3JHkzWhdVJGvO7zd6i/jNbSMym3uMMo0NO3HuFmQfuiOUJrcuxk9ZHSDyInLSJGdBkivK62NMDeR2t8fBVOEZuuo6E7LouygiNh2x+fn5o3769CHVTTbBqQK7ScU1I4It+VoXVOy+/gKN3Xgph9R9bJG/PqlXiY4CLS4DrG4CBJwEre90di2F0yMNXYfh243XQNbxb9ULoU7uo1CYxmkKiQYcnKJc9+wIuJcViHkdrrOztiU7Lz+PEvdeYffCu0SUHajyiHjlypEgeow5ltra2uH37Nk6fPi3Us06ePKkbK41UWJ363u72zoSweoYxUTYUePMA8N6ow+MwjO54HxkrGmSExcSjejFnTPnKLWngwOgRVInywhuwdADq/5zspYoFnfB7x8pieflpP2y/GgBjQmNHfeHCBUydOhUuLi4wNTUVj7p162LmzJn49lvuIa1tYfUftmdCWD2jUMlDy9lAu2VAtf66OQbD6JD4hEQM33gdT95EomBOGyF8Y2me6Rk9RipILEjV26HuSMD+02mLNpXzY0Qj5SibFAivPn0HY0HjXzSFth0cHMQyOevnz5+LZSrXunfvnvYtNFKyLKyeUUo3A6p0I4FY3eyfYXTI9H13cPZhMGwtzUR4NJe9ldQmMZmBOo+FPAMc8gE1025FPapJaTSv4IrYhEQMWn8Vz99HwRjQ+Ors5uaGGzduiOUaNWqIMq1z586JUXbx4sV1YaNRkilhdW2obBmh1iujn2y6/Axrzitb4f7ZpQrK5XOU2iQms1TqAnReD7T6XSkilM51cW7nKiib1wHB4crrYlSs4edFaeyox48fj0Sa9AeEc378+DHq1auH/fv3Y8GCBbqw0WjRSFg9q/idBBa6A/9xbTUjfy75vcGEXT5i+ftmNMoyPo1ig4JyCsp/BZRr89lN7azMRfSE2jD7BIbi++03dHdd1FdH3bx5c3z99ddiuWTJkrh79y6Cg4NFclmjRo10YaNRk2Fh9aziWAAIfwXcPwA8OaubYzCMFvB/G4khG66JBkE0bzmsoXLektFD6JpDin4aUsjZFst6esDCzEQIryw8/hCGjEaOOi4uTghv+Pgo72RVODs7c5alDqlVIhcmf1VBLFPJ1hFfHUi+uZRSlkQQh8crSyUYRmaQNCz18CYJxIoFnDC7QyW+9ugzh34B5lcB7ir7emtC9WLOmNbWTSzPPXIfB31ewFDRyFFbWFigcOHCXCstZ2H1rEAlEZb2wPPrwO0d2t8/w2QBagA0aos37gaFIbeDlQh/2liaSW0Wk5WcmKBbQNQ7wClzHSC7Vi+Mbz7UzI/acgO+z3VwXdTH0Pe4ceOEUtbbt291YxGjmbC6NqGSiDof5qiPTVE2RGEYmUCjJoomUfnVil4eokEQo8dQ0tjgc0DvXUA+ZY10ZhgvutC5ICouQURbKMkMxu6oFy1aJBqc5M+fH2XKlIG7u3uyB5N9wupD/rkqyre0Sq1hyhKJ98+Ayyu1u2+GySTU+GfRCeU85G8dKqJq4ZxSm8RoAzNzoHiDLO3C3MwUi7q5o5iLHQLf6+i6qG8tRNu1a6cbSxiNhNXbLzmPS4/fYvJ/tzGjnRY7MdFdbsNxwJ7hwOnfgao9ABu+KDLSccP/vah4IAbVL472VbNHKIfREQnxgPc/QOVugLl26t6dbC3EVEj7Jedw5ck7UREwq0NFg8lfYPUsmahnacrxuy/Rb62XmLOe2rYCetcqql2RjmV1gVe+QO0RQLPp2ts3w2jAy9BofLXoLF6GxqBR2TziYsza0nrO1TXAfyOB/FWBASeUpVla4uS9V/jfmiui5/vEL8vjf3WLwajVsxhpaVTWFT99EOyY8p+vdoXVTc2Aph/a+V1aDrx7qr19M0wGoQY/A9dfFU66VB57zO9ahZ20vhMbAZz49WOTEy2PeBuUyYNfPgh2TN/ni9P3X8MQ0NhRU29vMzOzNB+asnjxYhQtWhTW1tai09nly5fT3X7evHlibtzGxkbcjYwaNQrR0cnbawYGBqJnz57IlSuX2K5ixYrw8vKCoTHoi+L4umoBIfumdWH1kk2AYvWBhFjg+DTt7ZdhMgAF+n7+96YIe+ewtcCqPp5wMBAdYqPm/CIg/CWQsyjg2U8nh+hXtxg6ehQUo+rhG6/B73U4jG6OeufOnZ/UVl+/fh1r167FlClTNNrXli1bMHr0aCxbtkw4aXLC1FCFeobnyZPnk+03btyIn3/+GatXr0bt2rVx//59fPPNN2IeYu7cuWKbd+/eoU6dOmjYsCEOHDiA3Llz48GDB8iZ0/DmWelz//p1RfgFRwjhjv7rvLQnrE53us2mAcu/AG5tA2oOBQpwsiCTPSw75Ydd3s/FCHpJd0qgtJPaJCarhL0Ezs1XLjeepBQF0tF1cUZ7NzwOjhDCHVQhs3NYHTjZ6O+NntbmqMmJkuPdvXt3ht9DzrlatWoik5yg1qQ0Sh4xYoRwyCkZPnw47ty5g2PHjiU9N2bMGFy6dAlnzyq7adH7qPf4mTNnDHqOWp1XYh7vHIJCo9GwTG6s6lNNeyHCHQOBB4eBrxZmqL0fw2SVo74vMWC9Mv9iWtsK6KXN/AtGOvaOArxWAwU8gP7HtB72TsnrsBi0XXQWz0Oi8UXp3Fjdx1NkiBv1HHXNmjWTOdDPERsbi6tXr6JJkyYfjTE1FeskpZkaNIqm96jC435+fqLHeKtWrZK22bNnj9DG7tSpkxiVV61aFStXpl9mFBMTg9DQ0KRHWFgY9AkSVl/R2wNW5qZJwupao9kM4FtvdtJMtnD/ZRhGbr4unHSPGoXZSRsKr+8DV9cqlyk5NRuysXM7WGEFNcWxMBNz1TMPaPG6mM1oxVFHRUUJQY4CBQpk+D3UH5w6nLm6uiZ7ntaDglJXcOrevbsQAiH9a+qSVqJECTRo0EA0YFFBznvp0qUoVaoUDh06hCFDhgidbArNpwVpaTs5OSU9ypcvD32jUsEc+KPTR2H1f7UlrE5NUGxyaGdfDJMO7yJiRZiSGvrULO6c1DaXMQCOTgIUCUCZ1kCR2tl2WLcCTpjTWXld/OvsY2y94g+jcNQ010u9vVUPWid9apo3/v3336FLTp48iV9//RVLlizBtWvXsGPHDuzbtw/Tpn1MdqLwOTVeoe1oND1w4EAMGDBAzIOnxdixYxESEpL08PX1hT5CAgXDPwgUjN1xC9eeaVFYnYY4d/YCvhmf2mCYjBKXkIghG67i2dtIFHK2EUI01OCHMQCenAPu7QdMqJpEszwmbdCqYj6MbFxKLI/bdQteT94afjLZn3/+mayInMLVlLBF882aJGy5uLiILPGXL5MLTNB63rypS9ZNmDABvXr1Qv/+/cU6ZXNHREQIZ0ytTcmWfPnyfTIiLleuHP799980bbGyshIPFRT+1ldGNy0twoeHfV9i4Lqr+G9EHeRzssn6jm9tB3b0B+zzKjPCLTm5h9EeU/67jYt+b2FnaYa/+lQTEoaMAUA3+EcmKJc9vlGK/0jAyMalxHXxgE8QBv9zFbuH10WBHFq4LsrVUVOWtTawtLSEh4eHmNdWdTuj0TCtU9JYakRGRgpnrI6qJEyVE0cZ35Q1rg5lhxcpUgTGAAmr/9mlCjosPS/EC6j37bZBtbMuXkBasWfKAWVbKf/5GEZLrL/4FP9cfCamLed3rYrSrg5Sm8RoCxL3CbyqFPtp8GmCcHZeF+d0roynbyLh+yJUTLH8O6QWbC01doGSoHFs6e+//8a2bds+eZ6eS28eODWoNIsSveh9lM1N88k0Qu7bVym32Lt3bxGWVtGmTRsx/7x582Y8fvwYR44cEaNsel7lsKmu+uLFiyL0/fDhQ5GNvmLFCgwbNgzGQkph9R+0IaxOrf4GnwUaTwSs7LVlKmPknH8UjMl7bovlH5qXQZPyyXNWGD2GRH2Ofgh11xkJ2H9acpud2FqaY2UfT7jYW+LOi1CM2XpDKLLpBQoNKVWqlOL48eOfPH/y5ElF6dKlNd2dYuHChYrChQsrLC0tFdWrV1dcvHgx6bX69esr+vTpk7QeFxenmDx5sqJEiRIKa2trRaFChRRDhw5VvHv3Ltk+//vvP4Wbm5vCyspKUbZsWcWKFSs0ssnf35++PfFXn7n4KFhRYuw+RZGf9ioWHL0vtTkMk4wnweGKylMOid/nyE3XFImJiVKbxGiT2CiF4vQfCsVCT4UiJlwhF7yevFGU+mW/+N3NPXxPMjs08TMa11FTB7G7d++KbmLqPHnyRMwFUwa4vqNvddTpsfnyM/y845ZYXtbTAy3cUp//14inF4BjU4E284HcpbO+P8boCIuOw9dLzuPBq3BULuiELYNqwdqCtaUNEtIOoLbEMmKblz9++CD0sri7O1pXymdYddRUm3zzpvIDqnPjxg3RspORF+rC6qO3eouQT5Y5vwB4dh44Ojnr+2KMDmp5+91mb+GkXR2Vta7spA0YmTlpopNnIfT/INgxZps3fAJDIGc0dtTdunUTdcknTpwQddD0OH78OEaOHImuXbvqxkomS6iE1SNjE0QSRZaF1ZtMVpZa3NsHPD2vLTMZI+H3Q/dw7O4r0aBnRS9PuDpaS20So01Iy35lI+DhUciZsa3KoX7p3IiOS8TAdV6ik5nBOGqqWaZSrMaNGwvBC3o0a9YMjRo1EglcjPxIKaw+9J9rWRNWz10GcO+tXD48nrPAmQyz83oAlp16JJZnd6yEyoW4mY7BQTr2lOl9dp6srw1mpiZY0K0qiue2E21GB633Qkx8AgzCUVNZFfX0phKoDRs2iKYjjx49Eg1P6DVGnqiE1R2szXH5yVtM3O2TtUzwBmMBCzvlP+Tt5EItDJMa15+9w0//KvMlhjUsgbZVMt7JkNEjmkwBag5TivpkQ6vQrEBCHVS372htjmvP3mPczixeF3VEplv/UItO6qf95ZdfGk2Nsr5TMo89FnarCtLr2HzFH2vOP8n8zhxcgTrfKpePTVGWYjBMGgSJEctVEclpWt4VY5qWkdokRlfYOgMtfgXyV4U+UMzFDot7uIsR9varAaLVqN476g4dOuC333775PnZs2cLx83IG3Vh9Wl7fXHmQRaE1WsNB+xdgXdPgCt/ac9IxqCIjkvAwPVeeBUWgzKuDqIhDzWgYAyMyLeyDnWnR71SuUUuD/Hr/js4ce8V9NpRnz59OplalYqWLVuK1xj5oy6sPmxDFoTVqfFJww+CKKdnA1HvtWono/9QGJHKYG4GhIgGPKv6eMLeSj+6QTEalmCtaQ2sbaO8cddDvqldFF2rFRLXxW83XsfDV5m8LsrBUYeHh6c6F01qVvrcI9uYUAmrexTJidDoePRf54WQqLjM7axKT8ClDBD1Djg7V9umMnrOkpOP8N+N5zA3NcGSHu4o5GwrtUmMLvDeALzyBYJuAdZO0Nfr4tS2bqhe1BlhMfGi/XJIZCavi1I7ahLCoGSylFBbT32UhzRWrMzNRAOU/E7W8HsdgRGbriM+IROZ4GbmQNOpyuWLy5SlGQxDBQG3g0QpFkEXwJrFuc+CQRIbAZz4UPFT/0fAJuPiTHLD0twUS3u6C8GOx8ERGLbxWuaui1I7auqtTSVaffr0ET266UE9uadPny5eY/QHrQmrl24OFK0HJMQAx6dr20xGD7kbFIrvtniL5T61iqB7jcJSm8ToigtLgLAXQI4iQDWlsqE+k8veSkzR2Fqa4ezDYEzfd0f/HDUJYOzatUsIXgwdOhRjxoxBYGCgaHpSsqRSC5nRH7QirE4lGKpRtc8OIPS5lq1k9Ik34TGisQ412KlTMhcmfMmRNoMl/BVwbp5ymQR7SLzHACiXzxFzO1cRy1QdQ62YpSRT5VmtW7fGuXPnhNKVn58fOnfujO+//x6VKysv+Ix+oRVh9QLuQPOZwJDzgGN+7RvJ6AVUfjVkwzUEvItC0Vy2oo8yNdxhDJRTvwGx4UB+d6DC1zAkWrjlxZimSi2DCbt9cPlxJq6LWiLT/0GU4U3h7/z582POnDmiMxnJSzL6CTnqlm55EZegEMLq1MFMY2oNZZEOI8/wnrTntrigOViZi/BhDltugmSwBD8AvP5WLlNzE1PDuyEb3qgkvqyUL+m66P82UhI7NDqzQUFBmDVrVlKzE0dHR8TExIhQOD1frVo13VnKZIuwevl8jggOj/0QuozP/A5f31eWbDBGw7oLT7Hp8jMxE0KtGUvmcZDaJEaXkCiPIgEo3RIoWheGiImJCX7vWBkVCzjhbUSsyAQPj8nCdVHXjprmpsuUKSOUs+bNm4fnz59j4cKFurWOyVZstSWsTv2/l9QAvDfqwkxGhpx9EIype33F8tiWZdGwbB6pTWJ0CYnx3N0LmJgCTafAkLGxNMOK3h4i+fZuUBhGbfHO3HUxOxz1gQMH0K9fP0yZMkXMUZuZyU+6jMk6VJZAZVsWZiY44BOE+cceaL4T6lamSARe3NCFiYyMiIpNwB+H7qHvmstCvvJr9wIYUK+41GYxuoS6jx3+UOFD4jwk0mPg5HOywfJeHqJ864jvS8zLzHUxOxz12bNnERYWBg8PD6GetWjRIgQHB+vWOkYSPIs6Y0b7imKZHPW+my8020H1gUC/o0DrP3RjICML6ILVZO4pLDrxUMzhNSnnil/bVxThQsaA8d0FBHopRXkafOhMaAS4F86JWV9XRF5HazTO5ohRhh11zZo1sXLlSrx48QKDBg0SDU4okSwxMRFHjhwRTpwxHDp7FhKtRjMlrE4lGoU4X8FQoYSafmuuiPk6SjqkpjkUhVnZ2wPWFhxpM3iCfGj2VinKQ+I8RsTX7gVxdEz9bJdnNVFkQdOLpC7/+usvrF+/Hu/fv0fTpk2xZ88e6DsBAQEoVKgQ/P39UbBgQRgr1JHnf2u9RDMUuhjvHl5XzNNoREgA8OQcULmLrsxksgnS6l1xyk+MoGPiE0Vb0P71iuPbxiVFfgNjRLy4CTgXV/b7Z3TuZ7KUT0/JZaSaRQfctGlTVnbFyBCqf12YFWF1aie60APYNURZysHoLXSz1mLeGcw5cl846VrFc+Hgd/Xwc8uy7KSNkXyV2ElnI1opfKPEsnbt2hnEaJr5VFh9VW/PzAmr5ygMFKuvLOGgUg5G73gREiUU1nqvvix6H1NEZX7XKtg4oAaXXxkbt7YDbx5JbYVRYngV6ozWKZ7bHou6u4MkhDUWVqfWolTCQaUcTy/o0kxGi8QlJGLF6UdoPOcU9t16Ib77vnWK4tiY+mhbpQAnjBkb7/2BXUOBxdWBV9L3vjY22FEzGeKL0iSsXl5zYfU8ZYGqvZTLRyborbC8MXHJ7w1aLziDX/ffFf26SQ5174h6mNSmAhytLaQ2j5GCxHigeH2gUE0gd1mprTE62FEzGYZGVF08MyGs3vAXwMIWCLgC+O7WtZlMJnkdFoPRW7zRZcVF3H8ZDmc7S8zuUAnbBtVC+fyOUpvHSIlzMaDHNqDHVqUID5OtsKNmMgyFO6e1c0O1ojk1E1Z3yAvUHqFcprnq+Fid28pkHGpUsu7CEzSacxI7rgeK6zDJUh4fUx+dqxUS7WUZRmBpJ7UFRgk7aiYTwuoemgur1/4WsMsDvHsMXP3QyJ+RnOvP3qHt4rOYuPs2wqLj4VbAETuH1hGNS1hQg8HDY8DeUUDYS6ktMWrYUTMa42JvhZW9NRRWp1KOhmOVyydnAdEaNFBhtM67iFiM3XELXy89D5/AUDhYm2Na2wrYPawuqmRzMwdGppCoDrUK9VoNXGBdBylhR81kCpqz1FhYvWpvwKU0EPUWOPun7o1kPoHEBLZceSbC3KR0Rbl91J/7+JgG6FWrKMw4zM2ouLEJeHUbsHYC6o6W2hqjhh01kyVh9dGaCKubmQNNPijtXFyq7FrGZBu3n4eg47Lz+OnfW3gXGYcyrg7YOqiWuOHSuOMcY9jERgLHZyiX630P2DpLbZFRw46ayRIjGpVEa02E1cu0BIrUAeKjP14IGJ0SGh2HyXtuo83Cs6JpjZ2lGca1Koe939ZF9WJ8AWZS4eISIOw54FRYKbLDSAo7aibLmeB/dKwskpBUwuoR6QmrU0px02lAnvJAhfbZaarRQR3kdl0PFE1LaHqCyuropurYmAYY8EVxWJjxvz+TCuGvgbPzlMuNJwAW1lJbZPRwk15GO8LqvTzx1aJzScLqpKaUZllPQQ9g8DnAlB2FrnjwMkxMR1z0U05HFHOxw9S2FVCvVG6pTWPkzqnfgNgwIF9lwK2j1NYwPKJmtEX+HDZY0dsDlmamOOz7En8evZ/+G9SdNHcr0xoUzZh54A5azj8jnLS1hSm+b1ZaCGiwk2Y+S/DDj+WTFPnim2lZwN8Co1Vh9ZlfVxTLC48/xH83nqf/hrho4Nx8YG0bZSkIk6Uw90GfF2g69xSWn/JDfKICTcq54sio+hjeqBSszFknmskAxyYr24WWaqZsGcrIAlk46sWLF6No0aKwtrZGjRo1cPny5XS3nzdvnpDYtLGxEXqeo0aNQnR0dKrbzpo1S8yjfvfddzqynlGng0dBDPyiuFj+ftsN3ApIp146LhI4PQd4cga481/2GWlgPAmOwDd/X8Hgf64JOdKCOW2E4tmqPp4o5GwrtXmMvvDsovL/kER0SEyHkQ2Sz1Fv2bIFo0ePxrJly4STJifcvHlz3Lt3D3ny5Plk+40bN+Lnn3/G6tWrUbt2bdy/fx/ffPONcMZz585Ntu2VK1ewfPlyVKpUKRs/EfNTi7JijvTEvdciuWzP8DrI45hKQgqVfDSj8Jo5UK6NFKbqNdFxCVh68hGWnnqE2PhEMe0wqH5xDG1QUuQNMEyGoeknam5CVO0J5CkntUWMnEbU5FwHDBiAvn37onz58sJh29raCkecGufPn0edOnXQvXt3MQpv1qwZunXr9skoPDw8HD169MDKlSuRM2fObPo0DEFNM+Z3q4qSeewRFBqNgeuvCqeSKh59gKo9AFN2LJpw4u4rNPvzNOYfeyCcdL1SLmIeekyzMuykGc3xOwEEXFaK5zT4RWprGDk56tjYWFy9ehVNmjT5aJCpqVi/cCF17WIaRdN7VI7Zz88P+/fvR6tWrZJtN2zYMLRu3TrZvtMiJiYGoaGhSY+wsLAsfzZjh+QQKfzqZGMBb//3+GXHLTGP+tkmC9xaNF0C30dh0Hov9F1zBc/eRiKvozUWd3fHuv9VF7rhDJMpijcEOq1Vhrwd80ltDSOn0HdwcDASEhLg6uqa7Hlav3v3bqrvoZE0va9u3briwh8fH4/Bgwfjl18+3gVu3rwZ165dE6HvjDBz5kxMmfKhYxajNYq62GFpD3f0Wn1ZqDKVyeuAQfVLpL7xvYPK5v8UAm81O7tNlT00al511g8Ljz1EVFyCiFr0q1sM3zYuBXsryWewGH2H+htUaCe1FYxcQ9+acvLkSfz6669YsmSJcMY7duzAvn37MG3aNPG6v78/Ro4ciQ0bNojktIwwduxYhISEJD18fX11/CmMh9olXTCpTXmxPOvgXRy7k4YKj4WNshOS11/Am0fZa6TMOf8oGC3nn8bsg/eEk65e1Bn7v62HX1qVYyfNZI2YcCA6VGorGDk7ahcXF5iZmeHly+QXb1rPmzdvqu+ZMGECevXqhf79+6NixYpo3769cNw0Kk5MTBRh8VevXsHd3R3m5ubicerUKSxYsEAs0wg+JVZWVnB0dEx6ODg46OwzGyO9ahYR+sYU+R652Rv3X6YytUClIFQSQqUhpFnN4FVoNL7ddB3dV17Co9cRcLG3xNzOlbFlUE0RnWCYLHNmDrCgCnBru9SWMHJ11JaWlvDw8MCxY8eSniNnS+u1atVK9T2RkZFiHlsdcvYEhcIbN26MW7duwdvbO+nh6ekpEstoWbUtk31QRv6UryqgRjFnhMfEo/9aLyGz+Akk2EGlIXf2AP7pl+gZMqTvvfrsYzSacwp7bjwHNXjrXauIaP35tXtBcT4ZJstQ74KHR4DIN8qIFiNbJI+bUWlWnz59hDOtXr26KM+KiIgQWeBE7969UaBAATFiJtq0aSMyxatWrSrKuR4+fChG2fQ8OWEaDbu5uSU7hp2dHXLlyvXJ80z2QX2ll/b0QNvFZ0US1NAN17CuX/Xk/aZdywNVegDX1wOHxwP/O6ScOzMirj59i/G7buPOC2U4snKhHJjRzg1uBZykNo0xNKjSYsAJ4O5eoEzyZFxGXkjuqLt06YLXr19j4sSJCAoKQpUqVXDw4MGkBLNnz54lG0GPHz9ejCjob2BgIHLnzi2c9IwZrMQkd5ztLLGqdzV8veQcLvi9wdT/fDGtXYqbp4bjlGE4/0vKC4iR1Fe/CY/BbwfvYquXUvqTsuWpHr1rtUJp90xnmKxiZsHiOHqAieKzNTPGR0BAgOh4RolpBQsWlNocg+OI70sMXO8l5qynt3NDz5pFkm9wfDpw+nfAuQQw7JLyYmKgJCYqsOnKM5EoFhIVJ57r4lkIP7UsK25sGEYn+O4GSrcEzPk3pg9+Ru+yvhn9p2l5V3zfrIxYJp3kC4/eJN+gzkjALjfw9hFwdQ0MFWqv2n7peYzb6SOcdLl8jvh3SC381rESO2lGdzw6DmztDSwjXfhUckUY2cGOmpGEoQ1KoG2V/EI8YsiGq3j2JvLji1YOQIOflcsnZxlc+UhIZBwm7PLBV4vP4ob/ezhYmYsStv+G14FHEWepzWMMPYHs8ETlconGPKLWE9hRM5JAeQa/daiEygWd8D4yDv3XXUFYtDL0K3DvA+QqCUQGKxW2DACaZdp+NQCN5pzE+otPRei/XZX8ODamPvrWKQZz9cQ6htEFN7cCL28BVk5A/R+ltobJIHxlYCTD2sIMy3t5Io+DFe6/DMeoLd5ISPyQMkHz0lSuRVxYDIR+RjJT5twNCkXn5ReEotibiFjRB33jgBqY17Vq6oIlDKNt4qKU+R9EvdFKURxGL2BHzUhKXidrrOjtCUtzUxy98wp/HL738cWyrYHCtQBzK+DVHegjVDc+fa8vWi84iytP3sHGwgw/tywrOovVLuEitXmMMXFxKRAaADgWBGoMktoaRp/KsximSqEc+L1jJdG1jGQby7g6oF3VAsoa6nZLAJucyoeehbn33XqBaXt98TI0RjzXokJeTGhTHgVycHMJJpuJCAbO/qlcbjyBG5zoGeyoGVnQtkoB3A0KE476x39vCkEPcuBwLg5949HrcEzafRtnHwaL9SK5bEVntgZlPtVXZ5hsgcodY0KBvBWBip2ltobREHbUjGz4oVkZPHgZJkLgA9d5Yc/wuiI0LqDMq/sHlSPrwjUhR6JiE7D4xEMsP/0IcQkKEc6n7PbB9UuI+XiGkQQSubmySrncdBppCUttEaMh/I0xsoE6cFFyVWlXe7wKixFNUaLjPoioXFoObOoK7P+euoRAjk1cmsw9hUUnHgon3bBMbhwZ9QW+a1KanTQjLcemKMVuSjYBSjSU2homE7CjZmQFyTZSm9Gctha4GRCCH7ffFPO9qNgJsHdVXmwS5NOkwf9tJPqtuYIB67wQ+D4K+Z2ssaynB1Z/Uw1FctlJbR5j7PhfUXYhgwnQdKrU1jCZhEPfjOwonMsWS3p4oNdfl4R6FEk6DmtYEhh5E7CQRylTTHwCVpzyEyPomPhEWJiZoH+94hjRqCRsLfnfipEBdINL4jYEid24VpDaIiaT8BWFkSW1SuTC5K8qYPwuH1GyVdrVQbQelQOn77/GpD238Tg4QqzXKp4L09pVQMk8rBHNyKwLGUWg3voBjcZJbQ2TBTj0zcgWEuvoVbOIGBh8t/m6aBoiIK3qfzoCESl6hOuYFyFRGLbhGnqvviycNDVqmd+1imhcwk6akR1m5kD9H4BRPoBjfqmtYbIAO2pG1kxsU16MWCNiE9B/rRfehscA+8YoBe+p5CQbiEtIxIrTj9B4zilRG21maoL/1SkmWn9SWRm1Q2UY2UINgxi9hh01I2sszEyxpIe7qEUOeBeFIRuuIa7Rh9aiVHJCYT0dcsnvDVovOINf999FZGwCPIrkxH/D64obCAdrw5XfZPQYErFZ3x54dEJqSxgtwY6akT057SyxqrenyAi/9PgtJt3ODQUp/yTGAcd0k8n6OiwGo7d4o8uKi6IPOclOzu5YCdsG1UL5/I46OSbDaAXqjU9SlhR5SoiX2hpGC7CjZvSCUq4OWNCtiugquvHSM/znOlhZcnJ7JxDgpbXjkCjIugtPhMLVjuuB4njdaxTG8TH10dmzkKj1ZhhZU30gUGMI0Gyacp6a0XvYUTN6Q6OyrvipRVmxPOpkPF6W6KB8gUpQKOMsi1x/9g5tF5/FxN23ERYdj4oFnLBraB382r4ictiybi+jJ9jlAlrOUoraMAYB324xesWgL4rjflCYGO32fNQEh8z3wvTZBeDe/kxfmN5FxGL2oXvYfOWZ8PeO1ub4oUVZdK9eWCSOMYxeEBsBWNgqxWwYg4IdNaNXUIb1r19XhF9wBLz9gU2ObdAD24Ajk4BSzZQ61hkkMVGBbVf9MevAXbyLjBPPdXAviLGtysLFnjNlGT1jax9l177WcwCXUlJbw2gRDn0zegf1zl7RywN5Ha0xM7Q5Qk1zAG8eANfWZngft5+HoOOy8/jp31vCSZO05tZBtTCnc2V20oz+4XdSWbL49Bxgwpd1Q4O/UUYvyeNojRW9PRBnbo/fY9opnzw5C4gJS/d9odFxmLznNtosPItrz97DztIM41uXw95v66J6MefsMZ5htAmJ1ByeoFz27AfkKiG1RYyWYUfN6C2VCubAH50qY1NCI/gl5gUiXgPnFqS6LQl77LoeKJqWrDn/BIkK4MtK+XBsTAPRo5vqtRlGL7m1DQi6CVg5AvV/lNoaRgfwHDWj17SpnB/3gsrit1NdsdxyHhIuLIFZnW8Bq48tPUnjesJuH1z0eyvWi7vYYWpbN9Qt5SKh5QyjBeKigePTlMt1vwPs+DdtiLCjZvSe0U1LY3DQl1j64BGOmzXG/Chz5LcCImLiseD4A/x15jHiExWwtjDFiEal0L9eMViZs0Y0YwBcWgaE+AOOBYCaQ6W2htER7KgZvYeakPzZtSo6LB2Iu0FhGLjeCwO/KIFZ++/geUi02IaUtyZ+WR6FnG2lNpdhtEPkW+DMXOVyo/GAhY3UFjE6gh01YxDYWZljZW9PtF18Dj6BIbi7dTLmmt3E9zl/xZSvKqBxOVdg/w9AwBXNdlymtVKBiIgJB9Z+qVz+32HA/EMTlOPTgYdHNdtvkTpA8xkf11c1VbZE7b4VsM+jfO78IsBnu2b7zVMBaLf44/qGTsq5+69XfizZub4BuLJSs/3SiK3rho/r//YH3jwEWs4GClVXPndnL3DmD832a2EH9N33cV31HTUcB5RqqnzuydmPusqakNp3VHMYUKmT8rkgH2DPcM33m9p3VLk7UGOg8rmQAGBLT833m9p3lNbvj4h6B8SEAK5uQKUumh+P0RvYUTMGA42Wl3avirtrhqGn6SHEm9vi6Oj6opxLQI7l+XXNdkoXQRWKBLX3q3VCe/dU8/065Eu+/sJbWQOboKznFoQGar5f0xT/0uSMwp4DcZEfnwsP0ny/Ue+Tr7++CwTdAmI+SI8SkW803y8lQKmj+o5otKgiOkTz/ab1HUW8+vgcnZPM7De176h4g4/Pxcdkbr+pfUdp/v7UoFahpjyVY8iYKCgdlklGQEAAChUqBH9/fxQsWFBqcxgNefHoFqzDniKnvQ1QsvHHFwKuKp2JJpCOb163jxdolSIR7Vd1cXxxEwgL0my/lPRTwP3j+oOjgCIRKFbvYwjz1V3g/TPN9mvtBBSu8XHd75TScdBz9BpBimPBDzXbL9lEtql4ekFZCkefQZXARCPJl76a7ZfOYWrfEZ1zlYZy+CvguTc0JrXvKHdpIGfRjyNSfw0jLERq3xHtk/at6hD25Jzm+03tO0rr96fCwRXIV1nzYzF65WfYUacCO2qGYRhGLn6Gi0cZhmEYRsawo2YYhmEYGcOOmmEYhmFkDDtqhmEYhpEx7KgZhmEYRsZwHXUqJJIaDVV0vHghtSkMwzCMAaLyLyp/kx7sqFPh5cuX4m/16h86LjEMwzCMjvxN4cKF092G66hTIT4+HtevX4erqytMTbM2OxAWFoby5cvD19cXDg4fFZ2Y5PB5yjh8rjIGn6eMw+cq+88TjaTJSVetWhXm5umPmdlR65jQ0FA4OTkhJCQEjo4p2iUySfB5yjh8rjIGn6eMw+dK3ueJk8kYhmEYRsawo2YYhmEYGcOOWsdYWVlh0qRJ4i+TNnyeMg6fq4zB5ynj8LmS93niOWqGYRiGkTE8omYYhmEYGcOOmmEYhmFkDDtqhmEYhpEx7Kh1yOLFi1G0aFFYW1ujRo0auHz5stQmyY7Tp0+jTZs2yJ8/P0xMTLBr1y6pTZIlM2fORLVq1USThTx58qBdu3a4d++e1GbJkqVLl6JSpUqizpUetWrVwoEDB6Q2S/bMmjVL/A9+9913UpsiOyZPnizOjfqjbNmy2XZ8dtQ6YsuWLRg9erTIELx27RoqV66M5s2b49WrV1KbJisiIiLEuaGbGiZtTp06hWHDhuHixYs4cuQI4uLi0KxZM3H+mOQULFhQOJ2rV6/Cy8sLjRo1Qtu2bXH79m2pTZMtV65cwfLly8UNDpM6FSpUEP25VY+zZ88i26Csb0b7VK9eXTFs2LCk9YSEBEX+/PkVM2fOlNQuOUM/x507d0pthl7w6tUrcb5OnToltSl6Qc6cORWrVq2S2gxZEhYWpihVqpTiyJEjivr16ytGjhwptUmyY9KkSYrKlStLdnweUeuA2NhYcTffpEmTpOeoZzitX7hwQVLbGMOAWhgSzs7OUpsiaxISErB582YReaAQOPMpFKlp3bp1susV8ykPHjwQU3TFixdHjx498OzZM2QXrJ6lA4KDg8UFgkQ91KH1u3fvSmYXYxhQM3+aR6xTpw7c3NykNkeW3Lp1Szjm6Oho2NvbY+fOnUJMgUkO3cTQ1ByFvpm0oRyjNWvWoEyZMiLsPWXKFNSrVw8+Pj7ZImLCjpph9HAERBeIbJ0j0zPogurt7S0iD9u3b0efPn3EPD8764/4+/tj5MiRIueBEl6ZtGnZsmXSMs3jk+MuUqQItm7din79+kHXsKPWAS4uLjAzM0vStVZB63nz5pXMLkb/GT58OPbu3Suy5SlpikkdS0tLlCxZUix7eHiIEeP8+fNFwhSjhKbnKLnV3d096TmKBNJva9GiRYiJiRHXMeZTcuTIgdKlS+Phw4fIDniOWkcXCbo4HDt2LFm4ktZ5nozJDJRrR06aQrjHjx9HsWLFpDZJr6D/P3I8zEcaN24spggo8qB6eHp6ivlXWmYnnTbh4eF49OgR8uXLh+yAR9Q6gkqzKNxGP/zq1atj3rx5IqGlb9++Upsmux+8+l3p48ePxUWCkqQKFy4sqW1yC3dv3LgRu3fvFnNiQUFB4nnSxrWxsZHaPFkxduxYEaqk309YWJg4bydPnsShQ4ekNk1W0O8oZY6DnZ0dcuXKxbkPKfj+++9FvwcKdz9//lyU3dKNTLdu3ZAdsKPWEV26dMHr168xceJEcVGtUqUKDh48+EmCmbFDda4NGzZMdoND0E0OJW8wH5t4EA0aNEj2/N9//41vvvlGIqvkCYVze/fuLZJ+6EaG5hTJSTdt2lRq0xg9JSAgQDjlN2/eIHfu3Khbt67oaUDL2QGrZzEMwzCMjOE5aoZhGIaRMeyoGYZhGEbGsKNmGIZhGBnDjpphGIZhZAw7aoZhGIaRMeyoGYZhGEbGsKNmGIZhGBnDjpphGIZhZAw7aoZhshUTExPs2rVLajMYRm9gR80wRgS1GyVHmfLRokULqU1jGCYNuNc3wxgZ5JSpR7g6VlZWktnDMEz68IiaYYwMcsqki67+yJkzp3iNRtckAELqU6TKVbx4cWzfvj3Z+0kasVGjRuJ1UloaOHCgUEFTZ/Xq1ahQoYI4FkkBkkSnOsHBwWjfvj1sbW1RqlQp7NmzJ+m1d+/eCalFEjygY9DrKW8sGMaYYEfNMEwyJkyYgA4dOuDGjRvCYXbt2hV37twRr5FUa/PmzYVjv3LlCrZt24ajR48mc8Tk6EmWkxw4OXVywiVLlkx2jClTpqBz5864efMmWrVqJY7z9u3bpOP7+vriwIED4ri0PxcXl2w+CwwjI0g9i2EY46BPnz4KMzMzhZ2dXbLHjBkzxOt0SRg8eHCy99SoUUMxZMgQsbxixQpFzpw5FeHh4Umv79u3T2FqaqoICgoS6/nz51eMGzcuTRvoGOPHj09ap33RcwcOHBDrbdq0UfTt21fLn5xh9Beeo2YYI4P0v1X61iqcnZ2TlmvVqpXsNVr39vYWyzTCrVy5Muzs7JJer1OnDhITE3Hv3j0ROn/+/DkaN26crg2kEa2C9uXo6Ch0pIkhQ4aIEf21a9fQrFkztGvXDrVr187ip2YY/YUdNcMYGeQYU4aitQXNKWcECwuLZOvk4MnZEzQ//vTpU+zfvx9HjhwRTp9C6X/88YdObGYYucNz1AzDJOPixYufrJcrV04s01+au6a5ahXnzp2DqakpypQpAwcHBxQtWhTHjh3Lkg2USNanTx/8888/mDdvHlasWJGl/TGMPsMjaoYxMmJiYhAUFJTsOXNz86SELUoQ8/T0RN26dbFhwwZcvnwZf/31l3iNkr4mTZoknOjkyZPx+vVrjBgxAr169YKrq6vYhp4fPHgw8uTJI0bHYWFhwpnTdhlh4sSJ8PDwEFnjZOvevXuTbhQYxhhhR80wRsbBgwdFyZQ6NBq+e/duUkb25s2bMXToULHdpk2bUL58efEalVMdOnQII0eORLVq1cQ6zSfPnTs3aV/kxKOjo/Hnn3/i+++/FzcAHTt2zLB9lpaWGDt2LJ48eSJC6fXq1RP2MIyxYkIZZVIbwTCMPKC54p07d4oELoZh5AHPUTMMwzCMjGFHzTAMwzAyhueoGYZJgmfCGEZ+8IiaYRiGYWQMO2qGYRiGkTHsqBmGYRhGxrCjZhiGYRgZw46aYRiGYWQMO2qGYRiGkTHsqBmGYRhGxrCjZhiGYRgZw46aYRiGYSBf/g8dpeXwV0IDJAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 85.58%\n",
      "Validation accuracy: 88.59%\n",
      "Test accuracy: 86.33%\n"
     ]
    }
   ],
   "source": [
    "# Run over training, val and test sets\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 Using the LLM as a spam classifier (p 201)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to classify spam or not spam\n",
    "\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
    "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "# Test case 2\n",
    "\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for reuse (p202)\n",
    "#torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for reloading model in a new session (if needed)\n",
    "#model_state_dict = torch.load(\"review_classifier.pth\", map_location=device, weights_only=True)\n",
    "#model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch 7 - Fine Tuning To Follow Instructions (p 204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "# Load 1,100 instruction-response pairs (p 207)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # The book originally contained this unnecessary \"else\" clause:\n",
    "    #else:\n",
    "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #        text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# Print out a sample entry\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "# Another example without an input\n",
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (p 210)\n",
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset (p 213)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Confirm padding token\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom function (p 215) to pad to same length within each batch (not the whole dataset)\n",
    "\n",
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the function above works\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify custom collate to generate target token IDs from input token IDs (p 217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to modify custom collate so only one token has unknown token (50256) and the rest have -100 (p 220)\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Another test to confirm function working\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
